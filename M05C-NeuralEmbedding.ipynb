{
    "nbformat_minor": 2, 
    "cells": [
        {
            "source": "# Modern Data Science \n**(Module 05: Deep Learning)**\n\n---\n- Materials in this module include resources collected from various open-source online repositories.\n- You are free to use, change and distribute this package.\n\nPrepared by and for \n**Student Members** |\n2006-2018 [TULIP Lab](http://www.tulip.org.au), Australia\n\n---\n\n\n# Session A - Neural Embedding\n\n**The purpose of this session is to introduce vector space models (VSMs), called Word2Vec, that represent (embed) words in a continuous vector space where semantically similar words are mapped to nearby points. In this practical session, we present the following topics:****\n\n1. Learning vector representation of words using Gensim, a Python implementation of word2vec. \n2. Conducting an extrinsic evaluation of the word vectors through using them to build features in a sentiment classification of movie reviews.\n\n** References and additional reading and resources**\n- [Vector Representations of Words](https://www.tensorflow.org/tutorials/word2vec)\n- [Word2Vec word embedding tutorial in Python and TensorFlow](http://adventuresinmachinelearning.com/word2vec-tutorial-tensorflow/)\n- [Word2Vec Resources](http://mccormickml.com/2016/04/27/word2vec-resources/)\n\n\n---\n\n\n\n", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "## 0. Preliminaries", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "This section will instruct you to install the necessary software packages used in this notebook and initial introduction to datasets which are used throughout this practical session.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "### 0.1. Installing Gensim", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "Gensim is a free Python library designed to automatically extract semantic topics from documents. It is one of main libraries for handling textual data which is introduced in this and some consequence sessions. However, in this session, we only use Word2vec implementation from this library. \n\nTo install gensim, from the command line, you can run:\n\n``pip install gensim``\n\nHowever, you are also able to install packages in notetbooks using `!` notation before the commands as follows:\n", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "# you only need to run this command once to install gensim\n!pip install gensim", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "If Gesim is already installed, you will be noticed. \n\n<img src=\"https://raw.githubusercontent.com/tuliplab/mds/master/Jupyter/image/note.gif\" width=\"40\", align=\"left\"> Installing a new package might require administrator's previledge. If the above command fails, open an Annoconda command window with Administrator's right and type: \n\n                conda install gensim.\n", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "### 0.2. Datasets", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "We are mainly working on two datasets throughout this practical session: Wikipedia and IMDb movie reviews.\n\n1. **Wikipedia**: is the first 100,000,000 bytes (~100MB) of the English Wikipedia dump on Mar. 3, 2006, provided by Matt Mahoney at http://mattmahoney.net/dc/text8.zip\nThe data was already downloaded and stored at <span style=\"color:blue\">data/dl/wiki/wiki.txt</span>. If the file is not available, you can download and store in the corresponding folder.\n\n2. **IMDb movie reviews**: \nThis is a sentiment polarity dataset consisting of 25,000 movie reviews for training (*test-pos.txt and test-neg.txt* files) and 25,000 movie reviews for testing (*train-pos.txt and train-neg.txt* files). A half of these is labelled 'positive' and another half is labelled 'negative'. There are also 50,000 unlabelled data points (train-unsup.txt). It is downloaded from\nhttp://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\nhttp://ai.stanford.edu/~amaas/data/sentiment/\nThe data was already downloaded and stored at <span style=\"color:blue\">data/dl/sent</span>. If the files are not available, you can download and store in the corresponding folder.\n", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "## 1. Training a Word2Vec model", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "Word2Vec learns the semantic relationship between words. If you want to think of  Word2Vec as a magical blackbox, you can think that it takes words as input and returns a vector of numbers that represent the input word and its meaning. You can refer to the lecture 7 or [this tutorial](https://www.tensorflow.org/tutorials/word2vec) for details in how to implement a Word2Vec model in Tensorflow.\nWe can use Gensim library to train Wikipedia dataset to produces word embedded vectors and save the learned model into folder <span style=\"color:blue\"> model/wiki/</span>.\n\nFirst, importing necessary libraries:", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "#import Python libaries needed for training embbeded vectors \nfrom gensim.models.word2vec import LineSentence\nfrom gensim.models import Word2Vec\nimport time # for checking how long the training process takes", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "Indicating the input data location:", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "# where the data is located\ninput_data = 'data/wiki/wiki.txt'", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "Word2vec is a particularly computationally-efficient predictive model for learning word embeddings from raw text. It comes in two flavors, the **Continuous Bag-of-Words model (CBOW)** and the **Skip-Gram model**. Algorithmically, these models are similar, except that CBOW predicts target words (e.g. 'mat') from source context words ('the cat sits on the'), while the skip-gram does the inverse and predicts source context-words from the target words. This inversion might seem like an arbitrary choice, but statistically it has the effect that <span style=\"color:blue\">CBOW</span> smoothes over a lot of the distributional information (by treating an entire context as one observation). For the most part, this turns out to be a useful thing for <span style=\"color:blue\">smaller datasets</span> . However, <span style=\"color:yellowgreen\">skip-gram</span> treats each context-target pair as a new observation, and this tends to do better when we have <span style=\"color:yellowgreen\">larger datasets</span>.\n\n<img src=\"https://raw.githubusercontent.com/tuliplab/mds/master/Jupyter/image/note.gif\" width=\"40\", align=\"left\">  The following code may take time. You can read the following setion while waiting for the code be finished. You **must** create ``model/wiki`` folder to store the trained model. If there is no such folder, an error can be raised. In case you forgot to create the folder, you can run the code from ``model.save(model_file)`` to the end to save time.\n\nWhen training the model with given data, you can choose the corresponding algorithm as follows:", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "# parameters for training\nsg_ = 1 # the training algorithm. If sg=0, CBOW is used. Otherwise (sg=1), skip-gram is employed.\nalg = 'CBOW' if sg_ == 0 else 'sg'\nsize_ = 200 #  the dimensionality of the feature vectors\nwindow_ = 5 # the context size or the maximum distance between the current and predicted word\n\n# where to save the model learned\nmodel_file = 'model/wiki/' + alg + '_' + str(size_) + '_' + str(window_)\n\n# keep the time starting the training\nstart_time = time.time()\nprint(\"Running ...\")\n\n# training embedded vectors for the dataset with the parameters specified above\nmodel = Word2Vec(LineSentence(input_data), sg = sg_, size = size_, window = window_)\n# save the model learned into model file\nmodel.save(model_file)\n                 \n# show how long does it take to train the word vectors\nprint(\"=\"*40)\nruntime = time.time() - start_time\nprint(\"--- Running time: %s seconds ---\" % (runtime))", 
            "cell_type": "code", 
            "metadata": {
                "scrolled": true
            }, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "We can check the number of vocabularies in the corpus using the trained model:", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "words = list(model.wv.vocab.keys())\nprint(\"The number of words: {}\".format(len(words)))\nprint(\"The 10th word in the vocabularies: {}\".format(words[9]))\n", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "**<span style=\"color:red\">  Exercise 1: </span>**\n**<span style=\"color:#0b486b\"> \nTraining the embedded vectors for the IMDb movie reviews dataset, using only training data, stored at <span style=\"color:blue\"> data/sent/train-pos-neg-unsup.txt </span>\nSave the model learned in <span style=\"color:blue\"> model/sent/</span> \nUsing the same parameters as for training Wikipedia dataset above.\n</span>**", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "# Enter your own code here\n", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "## 2. Vector calculus\n", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "With the vectors at hand what will we do? Will they tell us how close *'dog'* and *'cat'* is? What word is closest to *'sister'*? Or Can we infer 'queen' from the vectors of *'king'*, *'woman'*, and *'man'*?\n\nIndeed, the proximity of two words can be computed as the cosine similarity of their vectors. Likewise, finding the closest word to a given word is in fact searching for the vector whose the largest cosine similarity.\n\nFor the last case, of all the vectors, model['queen'], vector for 'queen', should have the largest cosine similarity with v = model['woman'] + model['king'] - model['man']. \n\nThe following code show the list of words closed to some given word using vectorized representation of words. You can list 10 words similar to the word 'sister':\n", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "#import Python libaries needed \nfrom gensim.models import Word2Vec\nimport numpy as np\nfrom numpy import dot\nfrom numpy.linalg import norm\n\n# compute cosine similarity for two vector u & v\ndef cosine_similarity(u, v):\n    return dot(u, v)/(norm(u)*norm(v))\n\n# compute cosine similarity for vector v & all vectors in matrix W\ndef cosine_similarity_matrix(W, v):\n    return dot(W, v)/(norm(W, axis=1)*norm(v))\n\n\n# word vectors from the model in the order of words\nW = np.asarray([model[word] for word in words])\n\n# what is the words closest to a given word?\nword = 'sister'\n\n# vector of the word\nv = model[word]\n# its similarity to all of other vectors\nsim = cosine_similarity_matrix(W, v)\n# set the similar to its own zero as we do not want to see the word in the list\nsim[words.index(word)] = 0\n# indices of words whose the similarity from smallest to largest\nindices = sim.argsort()\n# reverse the order, then indices of words whose the similarity from largest to smallest\nindices = indices[::-1]\n# how many words you want\nTOP = 10 \nindices = indices[:TOP]\n# turn indices into words and their respective similarity to the word\ntop_words = [(words[i], sim[i]) for i in indices]\nprint(\"The top ten words similar to \\'{}\\'\".format(word))\ntop_words", 
            "cell_type": "code", 
            "metadata": {
                "scrolled": false
            }, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "**<span style=\"color:red\">  Exercise 2: </span>\nWhat are the ten most similar words to 'australia'? And the similarity scores? \n</span>**", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "# Enter your own code here\n", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "**<span style=\"color:red\">  Exercise 3: </span>\nWhat are the ten most similar words whose the vectors closest to (model['queen'] + model['man'] - model['king'])? And the similarity scores? Note that you have to set sim[words.index(word)] = 0 for all words in ['queen', 'man', 'king']\n</span>**", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "# Enter your own code here\n", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "## 3. Extrinsic evaluation of the word vectors\n\nIn this section we will use the word vectors learned from the previous sections to create a feature vector in a binary classification of movie reviews. For example, the review of 'I love the movie' will be represented as\n\n``( model['i'] + model['love'] + model['movie'] ) / 3``\n\ngiven that all but 'the' are existed in the learned word vectors.\n\nThree popular classifiers *DecisionTreeClassifier, RandomForestClassifier, GradientBoostingClassifier* will be used as the classfiers in the task. ", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "**<span style=\"color:#0b486b\"> \nThe following code evaluates how well the embedded vectors learned for the Wikipedia dataset contribute to the classification of the IDMb movie reviews </span>\n</span>**", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "#import Python libaries needed \nimport numpy as np\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nfrom gensim.models import Word2Vec\n\n# This function returns the word vector for an input review. It is computed as the average of all vectors, \n# if existed, for all words in the reviews\ndef get_avg_vector(review, model):\n    tokens = review.split()\n    vecs = [model[word] for word in tokens if word in model]\n    if len(vecs) > 0:\n        vecs = np.asarray(vecs).sum(0)/len(vecs)\n    return vecs\n\nclassifiers = [\n    DecisionTreeClassifier(),\n    RandomForestClassifier(),\n    GradientBoostingClassifier()\n]\n\n# re-load the saved model learned from Wikipedia data\nmodel_file = 'model/wiki/sg_200_5'\nmodel = Word2Vec.load(model_file)\n\n\n# Getting movie reviews data\nX_train = []\ny_train = []\nfor line in open('data/sent/train-pos.txt'):\n    vec = get_avg_vector(line, model)\n    if len(vec)>0:\n        X_train += [vec]\n        y_train += [1]\nfor line in open('data/sent/train-neg.txt'):\n    vec = get_avg_vector(line, model)\n    if len(vec)>0:\n        X_train += [vec]\n        y_train += [0]\n\nX_test = []\ny_test = []\nfor line in open('data/sent/test-pos.txt'):\n    vec = get_avg_vector(line, model)\n    if len(vec)>0:\n        X_test += [vec]\n        y_test += [1]\nfor line in open('data/sent/test-neg.txt'):\n    vec = get_avg_vector(line, model)\n    if len(vec)>0:\n        X_test += [vec]\n        y_test += [0]\n\nX_train, X_test, y_train, y_test = np.asarray(X_train), np.asarray(X_test), np.asarray(y_train), np.asarray(y_test)\n\nfor clf in classifiers:\n    clf.fit(X_train, y_train)\n    name = clf.__class__.__name__\n    train_predictions = clf.predict(X_test)\n    acc = accuracy_score(y_test, train_predictions)\n    print(\"Classifier: {}, Accuracy: {:.4%}\".format(name, acc))", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "**<span style=\"color:red\">  Exercise 4: </span>**\n**<span style=\"color:#0b486b\"> \nIn the above code, we use vector reprentation learned from Wikipedia data set for classification IDMb movie reviews. It seems not good. You now can load the model learned with IDMb movie reviews in **Exercise 1** and evaluates how well the embedded vectors learned for the IMDb movie reviews dataset contribute to the classification of the IDMb movie reviews.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "# Enter your own code here\n", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "**<span style=\"color:red\">  Exercise 5: </span>**\n**<span style=\"color:#0b486b\"> \nCompare with the performance in the classification of the word vectors learned for the Wikipedia dataset and explain why, if possible.\n</span>**", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "**Enter your explanation here**\n", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "## 4. Intrinsic evaluation of the word vectors\n\nIn this section we will evaluate how well the learned word vectors perform in predicting closest words in both semantic and syntactic context.\n\nReport the accuracy in this analogy prediction, against the file of data/questions-words.txt. There are 14 categories of analogies in the file, consisting of five semantic and nine syntactic analogies:", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "### 4.1 Semantic analogy\nConsists of the following sections:\n- capital-common-countries\n- capital-world\n- currency\n- city-in-state\n- family\n\n### 4.2 Syntactic analogy\nCmonsists of the following sections:\n- gram1-adjective-to-adverb\n- gram2-opposite\n- gram3-comparative\n- gram4-superlative\n- gram5-present-participle\n- gram6-nationality-adjective\n- gram7-past-tense\n- gram8-plural\n- gram9-plural-verbs", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "**<span style=\"color:#0b486b\"> \nThe following code evaluates how well the embedded vectors learned for the Wikipedia dataset match with analogy quadruples stored at <span style=\"color:blue\"> data/questions-words.txt </span>\n</span>**", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "#import Python libaries needed \nfrom gensim.models import Word2Vec\nimport numpy as np\n\n# load model learned above\nmodel_file = 'model/wiki/sg_200_5'\nmodel = Word2Vec.load(model_file)\n\n# evaluate how well the model matchs with the anology defined in data/questions-words.txt\nsections = model.accuracy('data/questions-words.txt', restrict_vocab=None)\n# This function returns the number of correct and incorrect matching when in predicting the hidden word in the quadruples of\n# [a, b, c, ?] words. These values are grouped by the 14 analogy categories. We will further group them into semantic and \n# syntactic  categories.\n\ntotal = np.zeros(2)\nsemantic = np.zeros(2)\nsyntactic = np.zeros(2)\nfor section in sections:\n    name = section['section'] \n    correct, incorrect = len(section['correct']), len(section['incorrect']) # len returns the number of matching\n    if not 'total' in name:\n        total += [correct, incorrect]\n        if 'gram' in name: # all syntactic section starts with 'gram'\n            syntactic += [correct, incorrect]\n        else: # otherwise, it's a semantic section\n            semantic += [correct, incorrect]\n            \ntotal = float( total[0]*100 ) / (sum(total) + 10.-6)\nsemantic = float( semantic[0]*100 ) / (sum(semantic) + 10.-6)\nsyntactic = float( syntactic[0]*100 ) / (sum(syntactic) + 10.-6)\n\nprint(\"=\"*80)\nprint('total accuracy: %0.2f%%, semantic accuracy: %0.2f%%, syntactic accuracy: %0.2f%%' % (total, semantic, syntactic))", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "**<span style=\"color:red\">  Exercise 6: </span>**\n**<span style=\"color:#0b486b\"> \nEvaluates how well the embedded vectors learned for the IMDb movie reviews dataset match with analogy quadruples stored at <span style=\"color:blue\"> data/questions-words.txt </span>\n</span>**", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "# Enter your own code here\n", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "**<span style=\"color:red\">  Exercise 7: </span>**\n**<span style=\"color:#0b486b\"> \nCompare with the performance of the word vectors learned for the Wikipedia dataset and explain why, if possible.\n</span>**", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "**Enter your explanation here**", 
            "cell_type": "markdown", 
            "metadata": {}
        }
    ], 
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3.5 with Spark 2.1", 
            "name": "python3-spark21", 
            "language": "python"
        }, 
        "language_info": {
            "mimetype": "text/x-python", 
            "nbconvert_exporter": "python", 
            "version": "3.5.4", 
            "name": "python", 
            "pygments_lexer": "ipython3", 
            "file_extension": ".py", 
            "codemirror_mode": {
                "version": 3, 
                "name": "ipython"
            }
        }, 
        "anaconda-cloud": {}
    }, 
    "nbformat": 4
}