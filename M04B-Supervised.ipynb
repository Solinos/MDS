{
    "nbformat_minor": 1, 
    "cells": [
        {
            "source": "# Modern Data Science \n**(Module 04: Machine Learning)**\n\n---\n- Materials in this module include resources collected from various open-source online repositories.\n- You are free to use, change and distribute this package.\n\nPrepared by and for \n**Student Members** |\n2006-2018 [TULIP Lab](http://www.tulip.org.au), Australia\n\n---\n\n\n# Session B - Supervised Learning\n\n\nThe purpose of this session is to demonstrate different coefficient and linear regression.\n\n\n## Content\n\n### Part 1 Data Dependency\n\n1.1 [Pearson's-r Correlation coefficient](#pearson)\n\n1.2 [Spearman's rank coefficient](#spearman)\n\n\n### Part 2 Linear Regression\n\n2.1 [Multiple Linear Regression](#mlr)\n\n2.2 [Regression for Median House Price](#rmhp)\n\n### Part 3 Distances\n\n3.1 [Euclidean Distance](#euclidean)\n\n3.2 [Cosine Distance](#cosine)\n\n3.3 [Term-by-Document Matrix](#t2d)\n\n### Part 4 K-NN Classification\n\n4.1 [K-NN in Python](#knn)\n\n4.2 [Decision Boundary](#db)\n\n### Part 5 Naive Bayes Classifier\n\n5.1 [NBC by Example](#nbc)\n\n5.2 [NBC Exercise](#nbc2)\n\n### Part 6 Random Forest\n\n6.1 [Decision Trees](#rf2)\n\n6.2 [Decision Trees and over-fitting](#rf3)\n\n6.3 [Ensembles of Estimators: Random Forests](#rf4)\n\n6.4 [Random Forest Regressor](#rf5)\n\n6.5 [Random Forest Limitations](#rf6)\n\n\n### Part 7 Confidence Interval\n\n7.1 [Population and Sample](#popsample)\n\n7.2 [Confidence Interval](#ci)\n\n\n---", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "---\n## <span style=\"color:#0b486b\">1. Data Dependency</span>\n\n<a id = \"pearson\"></a>\n\n\n### <span style=\"color:#0b486b\">1.1 Pearson's-r Correlation coefficient</span>\n\n\nWe assume $X=\\left\\{ X_{1},\\ldots,X_{n}\\right\\}$ \nand $Y=\\left\\{ Y_{1},\\ldots,Y_{n}\\right\\}$. Then Pearson-r correlation coefficient is defined as \n\n$$ \\rho(X,Y) = \\frac{\\text{cov}(X,Y)}{\\sigma_X \\sigma_Y} =  \\frac{\\sum_{i=1}^n (X_i - \\bar{X})(Y_i - \\bar{Y})}{\\sqrt{\\sum_{i=1}^n(X_i-\\bar{X})^2} \\sqrt{\\sum_{i=1}^n(Y_i-\\bar{Y})^2}} $$\n\nUse the car data and find the Pearson's-r correlation coefficient between car weights and fuel consumption.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "import numpy as np\nimport csv\nimport matplotlib.pyplot as plt\nimport scipy.stats\nimport pandas as pd\n\n%matplotlib inline", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "import wget\n\nlink_to_data = 'https://github.com/tuliplab/mds/raw/master/Jupyter/data/Auto.csv'\nDataSet = wget.download(link_to_data)", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "data = pd.read_csv('Auto.csv')", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "data.head()", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "data.describe()", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "data.head()", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "miles = data['miles']\nweights = data['Weight']", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "print miles[:10]\nprint weights[:10]", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "pearson_r = np.cov(miles, weights)[0, 1] / (miles.std() * weights.std())\nprint pearson_r", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "np.corrcoef(miles,weights)", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "horse = data['Horse power']", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "np.corrcoef(weights,horse)", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "# plotting\nfig, ax = plt.subplots(figsize=(7, 5), dpi=300)\nax.scatter(weights,miles, alpha=0.6, edgecolor='none', s=100)\nax.set_xlabel('Car Weight (tons)')\nax.set_ylabel('Miles Per Gallon')\n\nline_coef = np.polyfit(weights, miles, 1)\nxx = np.arange(1, 5, 0.1)\nyy = line_coef[0]*xx + line_coef[1]\n\nax.plot(xx, yy, 'r', lw=2)", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "**Exercise 1**: \n\n1. Find the Pearson's-r coefficient for two linearly dependent variables. Add some noise and see the effect of varying the noise. \n2. Simulate and visualize some data with positive linear correlation\n3. Simulate and visualize some data with negative linear correlation. ", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "xx = np.arange(-5, 5, 0.1)\npp = 1.5  # level of noise\nyy = xx + np.random.normal(0, pp, size=len(xx))\n\n# visualize the data\nfig, ax = plt.subplots()\nax.scatter(xx, yy, c='r', edgecolor='none')\nax.set_xlabel('X data')\nax.set_ylabel('Y data')\n\nline_coef = np.polyfit(xx, yy, 1)\nline_xx = np.arange(-5, 5, 0.1)\nline_yy = line_coef[0]*line_xx + line_coef[1]\n\nax.plot(line_xx, line_yy, 'b', lw=2)\n\nprint scipy.stats.pearsonr(xx, yy)", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "Pearson's r coefficient is limited to analyze the linear correlation between two variables. It is not capable to show the non-linear dependency. Investigate the Pearson's r coefficient between two variables that are correlated non-linearly.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "# generate some data, first for X\nxx = np.arange(-5, 5, 0.1)\n\n# assume Y = 2Y + some perturbation\npp = 1.1  # level of noise\nyy = xx**2 + np.random.normal(0, pp, size=len(xx))\n\n# visualize the data\nfig, ax = plt.subplots()\nax.scatter(xx, yy, c='r', edgecolor='b')\nax.set_xlabel('X data')\nax.set_ylabel('Y data')\nax.set_title('$Y = X^2+\\epsilon$', size=16)", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "# generate some data, first for X\nxx = np.arange(-5, 5, 0.1)\n\n# assume Y = 2Y + some perturbation\npp = 1.1  # level of noise\nyy = xx**2 + np.random.normal(0, pp, size=len(xx))\n\n# visualize the data\nfig, ax = plt.subplots()\nax.scatter(xx, yy, c='r', edgecolor='b')\nax.set_xlabel('X data')\nax.set_ylabel('Y data')\nax.set_title('$Y = X^2+\\epsilon$', size=16)", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "The Pearson's-r correlation is near zero which means there is no linear correlation. But how about non-linear correlation? Isn't $y=x^2$?", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "np.corrcoef(xx,yy)", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "<a id = \"spearman\"></a>\n\n\n### <span style=\"color:#0b486b\">1.2 Spearman's rank coefficient</span>\n\nSpearman's rank coefficient is used for discrete/ordinal data. Find the Spearman's rank between horse power and number of cylinders of the car data.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "data.head()", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "#horse = np.array([float(dd[4]) for dd in data[1:]])\n#cylinder = np.array([float(dd[2]) for dd in data[1:]])\nhorse = data['Horse power']\ncylinder = data['cylinder number']\n\n\nfig, ax = plt.subplots(figsize=(7, 5), dpi=300)\nax.scatter(horse, cylinder, alpha=0.6, edgecolor='none', s=100)\nax.set_xlabel('Horse power')\nax.set_ylabel('#Cylinders')\n\nprint scipy.stats.spearmanr(horse, cylinder)", 
            "cell_type": "code", 
            "metadata": {
                "scrolled": true
            }, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "**Exercise 2**. \nCompute the spearman rank correlation between \"Horse power\" and \"Engine displacement\"", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "displacement = data['Engine displacement']\nscipy.stats.spearmanr(horse,displacement)\n\nradius = 150*np.ones(len(horse))\nfig, ax = plt.subplots(figsize=(7,7),dpi=300)\nax.scatter(displacement, horse, alpha=0.2, c='m', edgecolor='none',s=radius)\n\nax.set_xlabel('Engine displacement')\nax.set_ylabel('Horse power')\nplt.savefig(\"Engine_vs_horse.pdf\")", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "---\n## <span style=\"color:#0b486b\">2. Linear Regression</span>\n", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "import numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\n%matplotlib inline", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "First we investigate a simple case by fitting a linear regression for three data points. First we simulate the data:", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "# simulating the data\n\nx = np.c_[0, 1, 2, 1.5].T\ny  = [1, 1.5, 3.1, 1.5]\n\nprint x\nprint y", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "#plotting the data\n\nfig, ax = plt.subplots(figsize=(5, 5), dpi=300)\nax.scatter(x, y, c='r')\nax.set_title('simulated data')\nax.set_xlabel('x')\nax.set_ylabel('y')", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "Now we fit the linear regression:", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "from sklearn import linear_model\n\n# instanciate the model\nlr = linear_model.LinearRegression()\n\n# fit the model\nlr.fit(x, y)", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "print \"Coefficients:\", lr.coef_\nprint \"   Intercept:\", lr.intercept_\nprint \"    Residues:\", lr.residues_", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "Let's plot the line to see how it estimates our data:", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "yhat = lr.predict(x)\n\nfig, ax = plt.subplots(figsize=(5, 5), dpi=300)\nax.scatter(x, y, c='r')\nax.plot(x, yhat)\n\nax.set_title('simulated data and the estimated line')\nax.set_xlabel('x')\nax.set_ylabel('y')", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "We can use the method `predict()` to predict `y` for a new `x`", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "x_test = np.c_[4, 2.3].T\ny_test = lr.predict(x_test)\n\nprint x_test.T\nprint y_test", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "<a id = \"mlr\"></a>\n\n\n### <span style=\"color:#0b486b\">2.1 Multiple Linear Regression</span>\n\n\nMultiple linear regression attempts to model the relationship between two or more explanatory variables and a response variable by fitting a linear equation to observed data. Every value of the independent variable x is associated with a value of the dependent variable y. For example if we have two explanatory variables (attributes, features), our data has such a form:\n\n$$\nD=\\left\\{ \\left(\\left(x_{1,1},x_{2,1}\\right),y_{1}\\right),\\left(\\left(x_{1,2},x_{2,2}\\right),y_{2}\\right),\\ldots,\\left(\\left(x_{1,n},x_{2,n}\\right),y_{n}\\right)\\right\\} \n$$\n\nNow we fit a multiple linear regression $y = x_1 + 2x_2 + 1$\n", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "# simulate the data\n\nx = np.c_[[0, 0], [0, 1], [1, 1], [1, 0]].T\ny = [1.5, 3.2, 4, 2]\n\nprint x\nprint y", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "mlr = linear_model.LinearRegression(fit_intercept=True)\nmlr.fit(x,y)", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "print mlr.coef_\nprint mlr.intercept_", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "print mlr.residues_", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "print mlr.predict(x)", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "**Exercises 3**: \n\nAs the score suggests, now we have the perfect regression. Change the values of $y$ slightly and see what effect it has on the `mlr`.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "<a id = \"rmhp\"></a>\n\n\n### <span style=\"color:#0b486b\">2.2 Regression for median house prices</span>\n", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "We are going to use the package `pandas` for reading and storing the data.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "data = pd.read_csv('data/housing_300.csv')", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "data.head()", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "data.describe()", 
            "cell_type": "code", 
            "metadata": {
                "scrolled": true
            }, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "Plot the scatter plot of the number of rooms vs the median house prices.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "fig, ax = plt.subplots(figsize=(7, 7), dpi=300)\nmedian_prices = data['MEDV']\navg_rooms = data['RM']\nscales = 50*np.ones(len(median_prices))\nax.scatter(avg_rooms, median_prices, color='b',s=scales, alpha=0.7, edgecolor='r')\nplt.xlabel('$X$ (number of rooms)')\nplt.ylabel('$Y$ (median house prices)')", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "print avg_rooms.shape\nprint median_prices.shape", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "How correlated are the number of rooms and the price of the house?", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "np.corrcoef(avg_rooms, median_prices)", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "Now we want to fit a linear regression mode on the data.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "# prepare the data\n\nx = np.c_[avg_rooms.values]\ny = median_prices.tolist()", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "from sklearn import linear_model\nlr = linear_model.LinearRegression()", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "lr.fit(x,y)", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "print lr.coef_\nprint lr.intercept_\nprint lr.residues_", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "# obtain the model parameters\n\nprint lr.coef_, lr.intercept_", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "# predict \n\nyhat = lr.predict(x)\nprint x[:10]\nprint yhat[:10]", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "#plot the result\n\nfig,ax = plt.subplots(figsize=(7,7),dpi=300)\n\nscales = 20*np.ones(len(median_prices))\nax.scatter(avg_rooms,median_prices,color='b',s=scales,alpha=0.7,edgecolor='r')\nplt.xlabel('$X$ (number of rooms)')\nplt.ylabel('$Y$ (median house prices)')\n\n# plot the regression linear leared\nax.plot(x,yhat)\n\n# visualize the residuals\ntmp = np.reshape(x,[1,len(x)])[0]\ntmp_x = []\ntmp_y = []\nfor i in xrange(len(x)):\n    tmp_x = np.append(tmp_x,tmp[i])\n    tmp_y = np.append(tmp_y,y[i])\n    tmp_x = np.append(tmp_x,tmp[i])\n    tmp_y = np.append(tmp_y,yhat[i])\n    ax.plot(tmp_x,tmp_y,color='g',linewidth=0.5)\n    tmp_x = []\n    tmp_y = []", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "check out the sum of residual:", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "lr.residues_", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "It is customary to test your model on **unseen** data. So we divide our data into two parts. We use 70% of it to train the model and 30% to evaluate its performance on unseen data.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "split = 0.7\nsplit_idx = int(np.round(split * len(data)))\nsplit_idx", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "train_data = data[0:200]\ntrain_data.head()", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "fig, axs = plt.subplots(1, 2, sharey=True)\ntrain_data.plot(kind='scatter', x='RM', y='MEDV', ax=axs[0], figsize=(7, 7))\ntrain_data.plot(kind='scatter', x='AGE', y='MEDV', ax=axs[1], figsize=(7, 7))", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "test_data = data[200:300]\ntest_data.head()", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "train_X = train_data['RM'].values\ntrain_X = np.c_[train_X]\ntrain_Y = train_data['MEDV'].tolist()\n\ntest_X = test_data['RM'].values\ntest_X = np.c_[test_X]\ntest_Y = test_data['MEDV'].tolist()", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "print type(train_X)\nprint train_X.shape\nprint type(train_Y)", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "'''\nBuild a linear regression model from training data\n'''\nfrom sklearn import linear_model\nlr = linear_model.LinearRegression()\n\nlr.fit(train_X, train_Y)", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "print lr.coef_\nprint lr.intercept_", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "Now we plot the linear regression result and the data to see how it fits the training data:", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "fig,ax = plt.subplots(figsize=(7,7),dpi=300)\n\n# plot training data\nscales = 20*np.ones(len(train_Y))\nax.scatter(train_X,train_Y,color='b',s=scales,alpha=0.7,edgecolor='r')\nplt.xlabel('$X$ (number of rooms)')\nplt.ylabel('$Y$ (median house prices)')\nplt.title('Training a simple linear regression model')\n\n# plot the regression line\ntrain_Yhat = lr.predict(train_X)\nplt.plot(train_X,train_Yhat)", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "Now that we have obtained the model parameters, we can use the model to predict for unseen data:", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "yhat_test = lr.predict(test_X)", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "fig,ax = plt.subplots(figsize=(7,7),dpi=300)\n\n# plot the predicted points along the prediction line\nscales = 30*np.ones(len(test_X))\nax.scatter(test_X,yhat_test,s=scales,color='b',edgecolor='r')\nax.plot(test_X,yhat_test,color='b',linewidth=.2)\n\n# plot the true values\nscales = 30*np.ones(len(test_X))\nax.scatter(test_X,test_Y,s=scales,color='g',edgecolor='b')\n\n# plot the residual line\ntmp = np.reshape(test_X,[1,len(test_X)])[0]\ntmp_x = []\ntmp_y = []\nfor i in xrange(len(test_X)):\n    tmp_x = np.append(tmp_x,tmp[i])\n    tmp_y = np.append(tmp_y,yhat_test[i])\n    tmp_x = np.append(tmp_x,tmp[i])\n    tmp_y = np.append(tmp_y,test_Y[i])\n    ax.plot(tmp_x,tmp_y,color='red',linewidth=0.5)\n    tmp_x = []\n    tmp_y = []", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "data.head()", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "---\n## <span style=\"color:#0b486b\">3. Distances</span>\n\n`Distance` is a numerical description of how far apart objects are. It is a concrete way of describing what it means for elements of some space to be close or far away from each other, for example the distance between two vectors in an 2-dimensional space.\n\nNow that you have know how to represent an n-dimensional vector in Python with NumPy arrays, we will write a function as a metric to measure the distance between two vectors. There are multiple ways to measure the distance between two vectors. We will discuss Euclidean distance and cosine distance.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "<a id = \"euclidean\"></a>\n\n\n### <span style=\"color:#0b486b\">3.1 Euclidean Distance</span>\n\nEuclidean distance comes from Geometry. If we assume $\\mathbf{x}_{1}=\\left[x_{11},x_{12},\\ldots,x_{1n}\\right]$ and $\\mathbf{x}_{2}=\\left[x_{21},x_{22},\\ldots,x_{2n}\\right]$, then the Euclidean distance between $\\mathbf{x}_{1}$ and $\\mathbf{x}_{2}$ is defined as:\n\n$$d\\left(\\mathbf{x}_{1},\\mathbf{x}_{2}\\right)=\\sqrt{\\left(x_{11}-x_{21}\\right)^{2}+\\left(x_{12}-x_{22}\\right)^{2}+\\ldots+\\left(x_{1n}-x_{2n}\\right)^{2}}\n$$\n\nWe can use array operators for this task.", 
            "cell_type": "markdown", 
            "metadata": {
                "collapsed": true
            }
        }, 
        {
            "source": "x1 = np.array([2, 5, 4, 6, 8])\nx2 = np.array([3, 5, 6, 8, 6])\n\nprint x1 - x2\nprint (x1 - x2) ** 2\nprint np.sqrt(np.sum((x1 - x2) ** 2))", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "import numpy as np", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "def euclidean_distance1(x1, x2):\n    d = x1 - x2\n    d = d ** 2\n    return np.sqrt(d.sum())", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "x1 = np.array([-1, 2, 0, 5])\nx2 = np.array([4, 2, 1, 0])\n\nprint euclidean_distance1(x1, x2)", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "Since two vectors passed to the function should be the same size, it is better to perform a sanity check before applying the subtraction. Otherwise it will raise an error. We can do this by using `if - elif` statement or as a better practice by using `try - except`.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "import sys\n\ndef euclidean_distance2(x1, x2):\n    if x1.shape[0] != x2.shape[0]:\n        sys.exit('x1 and x2 are not the same size')\n    else:\n        d = x1 - x2\n        d = d ** 2\n        return np.sqrt(d.sum())", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "# fix this cell\n\nx1 = np.array([-1, 2, 0, 5, 9])\nx2 = np.array([4, 2, 1, 0])\neuclidean_distance2(x1, x2)", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "def euclidean_distance3(x1, x2):\n    try:\n        d = x1 - x2\n        d = np.power(d, 2)\n        return np.sqrt(d.sum())\n    except ValueError as e:\n        print \"Vectors passed to the function are not the same size\"\n        # you can return a default value\n        return None", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "# fix this cell\n\nx1 = np.array([-1, 2, 0, 5, 9])\nx2 = np.array([4, 2, 1, 2])\na = euclidean_distance3(x1, x2)", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "def euclidean_distance4(x1, x2):\n    try:\n        d = np.array(x1) - np.array(x2)\n        d = np.power(d, 2)\n        return np.sqrt(d.sum())\n    except ValueError as e:\n        print \"Vectors passed to the function are not the same size\"\n        # you can return a default value\n        return None", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "<a id = \"cosine\"></a>\n\n### <span style=\"color:#0b486b\">3.2 cosine similarity and distance</span>\n\nCosine similarity is a measure of similarity between two vectors based on the angle between them. Cosine similarity is widely used in information retrieval and text mining as a measure of similarity between documents and is defined as:\n\n$$S_{c}\\left(\\mathbf{x}_{1},\\mathbf{x_{2}}\\right)=\\frac{\\mathbf{x}_{1}.\\mathbf{x_{2}}}{\\parallel\\mathbf{x}_{1}\\parallel^{2}+\\parallel\\mathbf{x}_{2}\\parallel^{2}-\\mathbf{x}_{1}.\\mathbf{x_{2}}}$$\n\n\nCosine similarity is particularly used in positive space where the outcome is bounded in [0, 1]. The cosine distance is defined as the complement to cosine similarity in positive space that is $D_{c}\\left(x_{1},x_{2}\\right)=1-S_{c}\\left(x_1,x_2\\right)$ where $D_c$ is the cosine distance and $S_c$ is the cosine similarity.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "x1 = np.array([1,2,3])\nx2 = np.array([3,4,6])\n\nx1 * x1", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "def cosine_distance(x1, x2):\n    try:\n        num = (x1*x2).sum()\n        denom = (x1*x1).sum() + (x2*x2).sum() - (x1*x2).sum()\n        num += 0.0    # or use np.astype(float) to make sure of float division\n        return 1 - num/denom\n    except ValueError as e:\n        print \"Vectors passed to the function are not the same size\"\n        return None\n    ", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "x1 = np.array([2, 0, 5, 9])\nx2 = np.array([4, 2, 1, 0])\ncosine_distance(x1, x2)", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "<a id = \"t2d\"></a>\n\n### <span style=\"color:#0b486b\">3.3 Term-by-Document matrix</span>\n\nA term-by-document matrix is a mathematical representation of a text corpus. It describes the frequency of terms that occur in the document collection. Each row corresponds to a document and each column correspond to a term. Thus the value that appears in row $j$ and column $i$ represents the frequency of appearing term $i$ in document $j$.\n\nWe will represent two datasets with term-by-document matrix:\n\n* a collection of 100 Twitter messages about Geelong\n* a collection of 6 news articles (5 about Apple and 1 about politics)\n\nThe data is already collected and stores in text files. Thus you will need to:\n\n* read the text files\n    * using file object\n* perform pre-processing\n    * using string methods\n    * using re package\n* construct the term-by-document matrix\n    * using numpy arrays and operations\n    ", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "### 3.3.1 Twitter dataset\nFirst read the data:", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "import os\n\n# get current working directory\ncwd = os.getcwd()   \n\n# join the subdirectory of the data and data file name\nfile_path = os.path.join(cwd, \"data/tweets.txt\")\n\n# read the contents of the file and store it in a list\nwith open(file_path) as fp:\n    tweets = fp.readlines()    \nfor tweet in tweets:\n    print tweet", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "Mostly when dealing with data, we have to perform some sort of data pre-processing. Data collection is often loosely controlled, resulting in out of the range values, missing values, and etc. Thus quality of the data is first and formost before running an analysis. This step is specific to the nature of the data. For example for text data it may consist of cleaning, normalization, tokenization, and etc. \n\nIn this case, our pre-processing consists of:\n\n* converting all the words into lower case to remove the effect of the letter case\n* replacing the URLs with a simple string such as 'url'. From the previous cell, you should be able to see that many of the tweets contain a URL. Since we are not using them now, we can remove them or replace them.\n* Removing the punctuations", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "import numpy as np\nimport re\nimport string\nfrom collections import Counter\n\ndef pre_process(doc):\n    \"\"\"\n    pre-processes a doc\n      * Converts the tweet into lower case,\n      * removes the URLs,\n      * removes the punctuations\n      * tokenizes the tweet\n    \"\"\"\n    \n    doc = doc.lower()\n    # gettign rid of non ascii codes\n    doc = doc.decode('ascii', 'ignore')\n    \n    # repalcing URLs\n    url_pattern = \"http://[^\\s]+|https://[^\\s]+|www.[^\\s]+|[^\\s]+\\.com|bit.ly/[^\\s]+\"\n    doc = re.sub(url_pattern, 'url', doc) \n\n    punctuation = r\"\\(|\\)|#|\\'|\\\"|-|:|\\\\|\\/|!|_|,|=|;|>|<|\\.\"\n    doc = re.sub(punctuation, ' ', doc)\n    \n    return doc.split()", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "print r['text']\npre_process(r['text'])", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "**Exercise:**\n\nUse the function provided to pre-process one of the tweets.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "# code here", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "<div align=\"right\">\n<a href=\"#matmul1\" class=\"btn btn-default\" data-toggle=\"collapse\">Click here for the solution</a>\n</div>\n<div id=\"matmul1\" class=\"collapse\">\n```\nTBA\n```\n</div>", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "def termdoc(docs):\n    \"\"\"\n    returns the term-by-document matrix and the vocabulary of the passed corpus\n    \"\"\"\n    \n    vocab = set()   \n    termdoc_sparse = []\n\n    for doc in docs:\n        # pre-process the doc\n        doc_tokens = pre_process(doc)\n        # computes the frequencies for doc\n        doc_sparse = Counter(doc_tokens)    \n\n        termdoc_sparse.append(doc_sparse)\n\n        # update the vocab\n        vocab.update(doc_sparse.iterkeys())  \n\n    vocab = list(vocab)\n    vocab.sort()\n\n    n_docs = len(docs)\n    n_vocab = len(vocab)\n    termdoc_dense = np.zeros((n_docs, n_vocab), dtype=int)\n\n    for j, doc_sparse in enumerate(termdoc_sparse):\n        for term, freq in doc_sparse.iteritems():\n            termdoc_dense[j, vocab.index(term)] = freq\n            \n    return termdoc_dense, vocab", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "tweets_termdoc, tweets_vocab = termdoc(tweets)", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "Lets look at the vocabulary:", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "Lets look at one of tweets:", 
            "cell_type": "markdown", 
            "metadata": {
                "collapsed": true
            }
        }, 
        {
            "source": "j = 0\nprint tweets[j]\nprint tweets_termdoc[j]", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "tweets_vocab.index('beyond')\n", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "tweets_termdoc[j][127]", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "So baiscally, now each tweet is represented by a vector of size `len(tweets_vocab)`.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "### 3.3.2 News dataset\nSimilar to previous sections, the data is stored in text files names as'news1.txt', ..., 'news5.txt'. All we have to do is read the files, construct the corpus and send it to `termdoc()` function.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "n_docs = 6\ncwd = os.getcwd()   \nnews = []\n\nfor j in xrange(1, n_docs+1):\n    filename = \"news{}.txt\".format(j)\n    file_path = os.path.join(cwd, \"data/{}\".format(filename))\n    with open(file_path) as fp:\n        news.append(fp.read())\n\nnews_termdoc, news_vocab = termdoc(news)", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "So now each news article is represented with a large vector of size `len(news_vocab)`. We can do many things with this representation. For example measuring the distance between two documents. The first 5 news articles are tech news and about Apple, but the 6th one is about politics. We expect that tech news be more similar to each other rather than to the politics news. In other words the distance between two articles from tech news should be less than the distance between a tech news article and a news article about politics. This is shown below:", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "print cosine_distance(news_termdoc[1], news_termdoc[2])   # both aout Apple\nprint cosine_distance(news_termdoc[1], news_termdoc[4])   # one from tech world, the other from politics", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "---\n### <span style=\"color:#0b486b\">4. k-Nearest Neighbours Classification</span> \n\nkNN is a non-parametric classification technique which is extensively used in practice. Its input consists of the `k` closest training examples and the output is a class membership. An object is classified by a majority vote of its neighbors, with the object being assigned to the class most common among its `k` nearest neighbors (k is a positive integer, typically small). If k = 1, then the object is simply assigned to the class of that single nearest neighbor.\n\n**Please note that kNN is different from K-means.** K-means is a clustering algorithm that tries to partition a set of points into K sets (clusters) such that the points in each cluster be close to each other. It is unsupervised because the points have no external classification. kNN is a classification algorithm that in order to determine the classification of a point, combines the class of the k nearest points. It is supervised because you are trying to classify a point based on the known label of other points.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "<a id = \"knn\"></a>\n\n### <span style=\"color:#0b486b\">4.1 kNN in Python</span> \n\nTo be able to illustrate how we perform kNN classification in Python, we need some data first. Therefore we synthesize some data from 3 classes. We assume the data in each class comes from a multivariate random distribution.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "import numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\nsns.set()", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "np.random.seed(100)\nn_per_class = 50\ncolors = ['green', 'blue', 'magenta']\n\nmean1 = [-5, 10]\ncov1 = [[1.5, 0], [0, 1.5]]\nmean2 = [0, 7]\ncov2 = [[1.5, 0], [0, 3]]\nmean3 = [-6, 6]\ncov3 = [[2, 0], [0, 1.5]]\n\nmeans = [mean1, mean2, mean3]\ncovs = [cov1, cov2, cov3]\n\nx11, x12 = np.random.multivariate_normal(means[0], covs[0], n_per_class).T\nx21, x22 = np.random.multivariate_normal(means[1], covs[1], n_per_class).T\nx31, x32 = np.random.multivariate_normal(means[2], covs[2], n_per_class).T\n\nscale = 75\nalpha = 0.6\n\nfig, ax  = plt.subplots(figsize=(7, 7), dpi=300)\nax.scatter(x11, x12, alpha=alpha, color=colors[0], s=scale)\nax.scatter(x21, x22, alpha=alpha, color=colors[1], s=scale)\nax.scatter(x31, x32, alpha=alpha, color=colors[2], s=scale)\n\nax.set_title(\"synthesized data for 3 classes\")\nax.set_xlabel(\"$x_1$\")\nax.set_ylabel(\"$x_2$\")", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "Then we have to instantiate a kNN classifier from sklearn.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "from sklearn import neighbors\n\nweights='uniform'\nk = 15\nknn = neighbors.KNeighborsClassifier(k,weights=weights)", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "We need to pass one array as training features and on array as training labels to the `knn` object. Therefore we have to put all the attributes together (also class labels).", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "x1 = np.r_[x11, x21, x31]\nx2 = np.r_[x12, x22, x32]\nX_train = np.c_[x1, x2]", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "Y_train = np.r_[0*np.ones(n_per_class), 1*np.ones(n_per_class), 2*np.ones(n_per_class)]", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "Now we can fit the model", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "knn.fit(X_train, Y_train)", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "Now we will see how kNN classifies a point.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "k = 1\nknn = neighbors.KNeighborsClassifier(k)\nknn.fit(X_train, Y_train)", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "from matplotlib.colors import ListedColormap\ncmap_bold = ListedColormap(['green', 'blue', 'magenta'])", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "fig,ax = plt.subplots(figsize=(7, 7))\n\nax.scatter(X_train[:, 0], X_train[:, 1], c=Y_train, cmap=cmap_bold, alpha=alpha, s=scale)\n\nplt.title(\"3-Class classification (k = {})\".format(k))\n\nX_test = [-7, 10]\nY_pred = knn.predict(X_test)\nax.scatter(X_test[0], X_test[1], marker=\"x\", s=scale, lw=2, c='k')\n\nax.set_title(\"3-Class classification (k = {})\\n Red point is predicted as class {}\".format(k, colors[Y_pred.astype(int)[0]]))", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "<a id = \"db\"></a>\n\n### <span style=\"color:#0b486b\">4.2 Decision Boundry</span> ", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "kNN effectively partitions the feature space into different sets and assigns the same class label to points belonging to the same partition. This partitioning changes as we change k. We illustrate this below. As you see bigger values of k, partition the space more smoothly.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "from matplotlib.colors import ListedColormap", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "k = 15\nknn = neighbors.KNeighborsClassifier(k, weights=weights)\nknn.fit(X_train, Y_train)\n\n# step size in the mesh\nh = 0.05\n\n# Create colour maps\ncmap_light = ListedColormap(['#AAFFAA', '#AAAAFF', '#FFAAAA'])\ncmap_bold = ListedColormap(['green', 'blue', 'magenta'])\n\nx1_min, x1_max = X_train[:, 0].min() - 1, X_train[:, 0].max() + 1\nx2_min, x2_max = X_train[:, 1].min() - 1, X_train[:, 1].max() + 1\nxx1, xx2 = np.meshgrid(np.arange(x1_min, x1_max, h), np.arange(x2_min, x2_max, h))\n\nZ = knn.predict(np.c_[xx1.ravel(), xx2.ravel()])\n\n# Put the result into a color plot\nZ = Z.reshape(xx1.shape)\n\nfig,ax = plt.subplots(figsize=(7, 7))\nax.pcolormesh(xx1, xx2, Z, cmap=cmap_light)\n\n# Plot also the training points\nax.scatter(X_train[:, 0], X_train[:, 1], c=Y_train, cmap=cmap_bold, alpha=alpha, s=scale)\n\nplt.xlim(xx1.min(), xx1.max())\nplt.ylim(xx2.min(), xx2.max())\nplt.title(\"3-Class classification (k = {})\".format(k))", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "Now we will investigate the effect of `'k'` on decision boundaries. Lets train a classifier with `k=1` which means we only use the label of the closest point to predict the label of a test point.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "k = 1\nknn = neighbors.KNeighborsClassifier(k, weights=weights)\nknn.fit(X_train, Y_train)\n\nZ = knn.predict(np.c_[xx1.ravel(), xx2.ravel()])\nZ = Z.reshape(xx1.shape)\n\nfig,ax = plt.subplots(figsize=(7, 7))\nax.pcolormesh(xx1, xx2, Z, cmap=cmap_light)\n\nax.scatter(X_train[:, 0], X_train[:, 1], c=Y_train, cmap=cmap_bold, alpha=alpha, s=scale)\n\nplt.xlim(xx1.min(), xx1.max())\nplt.ylim(xx2.min(), xx2.max())\nplt.title(\"3-Class classification (k = {})\".format(k))", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "k = 2\nknn = neighbors.KNeighborsClassifier(k, weights=weights)\nknn.fit(X_train, Y_train)\n\nZ = knn.predict(np.c_[xx1.ravel(), xx2.ravel()])\nZ = Z.reshape(xx1.shape)\n\nfig,ax = plt.subplots(figsize=(7, 7))\nax.pcolormesh(xx1, xx2, Z, cmap=cmap_light)\n\nax.scatter(X_train[:, 0], X_train[:, 1], c=Y_train, cmap=cmap_bold, alpha=alpha, s=scale)\n\nplt.xlim(xx1.min(), xx1.max())\nplt.ylim(xx2.min(), xx2.max())\nplt.title(\"3-Class classification (k = {})\".format(k))", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "k = 3\nknn = neighbors.KNeighborsClassifier(k, weights=weights)\nknn.fit(X_train, Y_train)\n\nZ = knn.predict(np.c_[xx1.ravel(), xx2.ravel()])\nZ = Z.reshape(xx1.shape)\n\nfig,ax = plt.subplots(figsize=(7, 7))\nax.pcolormesh(xx1, xx2, Z, cmap=cmap_light)\n\nax.scatter(X_train[:, 0], X_train[:, 1], c=Y_train, cmap=cmap_bold, alpha=alpha, s=scale)\n\nplt.xlim(xx1.min(), xx1.max())\nplt.ylim(xx2.min(), xx2.max())\nplt.title(\"3-Class classification (k = {})\".format(k))", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "k = 5\nknn = neighbors.KNeighborsClassifier(k, weights=weights)\nknn.fit(X_train, Y_train)\n\nZ = knn.predict(np.c_[xx1.ravel(), xx2.ravel()])\nZ = Z.reshape(xx1.shape)\n\nfig,ax = plt.subplots(figsize=(7, 7))\nax.pcolormesh(xx1, xx2, Z, cmap=cmap_light)\n\nax.scatter(X_train[:, 0], X_train[:, 1], c=Y_train, cmap=cmap_bold, alpha=alpha, s=scale)\n\nplt.xlim(xx1.min(), xx1.max())\nplt.ylim(xx2.min(), xx2.max())\nplt.title(\"3-Class classification (k = {})\".format(k))", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "#### 4.2.1 Prediction\n\nPlay with the `X_test` and `k` to see how the classifier behaves.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "k = 5\nknn = neighbors.KNeighborsClassifier(k, weights=weights)\nknn.fit(X_train, Y_train)\n\nZ = knn.predict(np.c_[xx1.ravel(), xx2.ravel()])\nZ = Z.reshape(xx1.shape)\n\nfig,ax = plt.subplots(figsize=(7, 7))\nax.pcolormesh(xx1, xx2, Z, cmap=cmap_light)\n\nax.scatter(X_train[:, 0], X_train[:, 1], c=Y_train, cmap=cmap_bold, alpha=alpha, s=scale)\n\nplt.xlim(xx1.min(), xx1.max())\nplt.ylim(xx2.min(), xx2.max())\nplt.title(\"3-Class classification (k = {})\".format(k))\n\nX_test = [-4, 8]\nY_pred = knn.predict(X_test)\nax.scatter(X_test[0], X_test[1], alpha=0.95, color='r', s=3*scale)\n\nax.set_title(\"3-Class classification (k = {})\\n Red point is predicted as class {}\".format(k, colors[Y_pred.astype(int)[0]]))", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "Now instead of predicting the class label for one point, we use our model to predict the labels of multiple points.\n\nFirst we generate some test data from the first class. This way we know the true class labels. Then we can use the `kNN` classifier to predict labels for the test data and get the predicted class labels. A measure of  accuracy for the classifier can be defined by comparing the true and predicted labels.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "n_test = 100\nX1_test, X2_test = np.random.multivariate_normal(mean1, cov1, n_test).T\nY_true = 0 * np.ones(n_test)", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "X_test = np.c_[X1_test, X2_test]", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "Y_pred = knn.predict(X_test)", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "Y_pred", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "How many times the classifier predicts the labels correctly?", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "Y_pred == Y_true", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": " Accuracy:", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "(sum(Y_pred == Y_true) + 0.0) / n_test", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "**Exercise**: Repeat the previous experiment with a classifier which has been trained with a different `k`.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "**Exercise**: Use kNN to implement a classifier on handwritten digits dataset introduced in prac7.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "from sklearn import datasets", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "digits = datasets.load_digits()\n\ndigits.keys()", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "digits['target']", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "digits['data']", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "X = digits['data']\nY = digits['target']", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "from sklearn.cross_validation import train_test_split", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.3)", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "from sklearn import neighbors\n\nk = 15\nknn = neighbors.KNeighborsClassifier(k)", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "knn.fit(X_train, Y_train)", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "Y_pred = knn.predict(X_test)", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "Y_pred == Y_test", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "(sum(Y_pred == Y_test) + 0.0) / len(Y_test)", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "mask = Y_pred != Y_test", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "fig, axes = plt.subplots(nrows=1, ncols=10, figsize=(10, 1))\nfor i, ax in enumerate(axes.ravel()):\n    ax.imshow(X_test[mask][i, :].reshape(8, 8))\n    ax.set_xticklabels([])\n    ax.set_yticklabels([])\n    ax.set_xticks([])\n    ax.set_yticks([])\n    ax.set_title(\"{}\".format(Y_pred[mask][i]))", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "## <span style=\"color:#0b486b\">5. Naive Bayes Classifier</span> \n\n\nNaive Bayes is one of the most practical classification machine learning algorithms. \n\n* fast\n* good performance\n* simple yet very effective\n* robust to irrelative features\n\nSo why is it called naive?\n\nBecause it does not consider the dependency between features and assume all features are independent of each other which is not the case in reality. This is a naive assumption, hence the name.\n\nThe accuracy is very good although this naive assumption. A famous example of NB usage is spam filtering.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "<a id = \"nbc\"></a>\n### <span style=\"color:#0b486b\">5.1 NBC by Example</span> \n\nWe assume we have collected the below data for the past 5 days. Based on this data, can we predict if our subject will play in a setting like:\n\n    outlook  = overcast\n    temp     = hot\n    humidity = normal\n    windy    = no", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "<!-- <img src=\"nb_data.png\" width=\"800\"> -->\n<img src=\"https://github.com/tuliplab/mds/raw/master/Jupyter/image/nb_data.png\" width=\"800\">\n<br />", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "First we have to find a representation for our data. We can construct a dictionary to convert stings into numbers and then save them in a dataframe. \n\n    outlook: sunny=0, overcast=1, rainy=2\n    temp: hot=0, mild=1, cool=2\n    humidity: normal=0, high=1\n    wind: no=0, yes=1\n    play: np=0, yes=1", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "from __future__ import division\n\nimport numpy as np\nimport pandas as pd", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "data = {\n    'outlook': [0, 1, 2, 0, 1],\n    'temp'   : [0, 1, 2, 1, 0],\n    'humid'  : [0, 0, 1, 0, 1],\n    'wind'   : [0, 0, 1, 1, 0],\n    'play'   : [1, 1, 0, 0, 0,]    \n}\n\ndf = pd.DataFrame(data)", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "Now we use Bayes rule to construct a Naive Bayes classifier. We can write:\n\n$$Pr\\left(p|o,t,h,w\\right)\\propto Pr\\left(p\\right)Pr(o|p)Pr(t|p)Pr(h|p)Pr(w|p)$$", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "To calculate $Pr(p)$ we use marginal probablity.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "def marginal_prob(df, col):\n    ll = [(ss, (df[col] == ss).sum()) for ss in set(df[col])]\n    total_count = [b for a,b in ll]\n    total_count = sum(total_count)\n    \n    ll2 = [(a, b/total_count) for a, b in ll]\n    return dict(ll2)", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "To calculate probability of a feature given the class (play) we use conditinoal probability.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "def conditional_prob(df, f, c, val):\n    df2 = df[df[c] == val][f]\n    ll = [[ss, (df2 == ss).sum()] for ss in set(df2)]\n    total_count = [b for a,b in ll]\n    total_count = sum(total_count)\n    \n    ll2 = [(a, b/total_count) for a, b in ll]\n    return dict(ll2)", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "Now we can use Bayes rule:", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "o = 1\nt = 0\nh = 0\nw = 0\n\nc = 0\np0 = marginal_prob(df, 'play')[c] * conditional_prob(df, 'outlook', 'play', c)[o] * conditional_prob(df, 'temp', 'play', c)[t] \\\n* conditional_prob(df, 'humid', 'play', c)[h] * conditional_prob(df, 'wind', 'play', c)[w]\n\nc = 1\np1 = marginal_prob(df, 'play')[c] * conditional_prob(df, 'outlook', 'play', c)[o] * conditional_prob(df, 'temp', 'play', c)[t] \\\n* conditional_prob(df, 'humid', 'play', c)[h] * conditional_prob(df, 'wind', 'play', c)[w]\n\n# normalizing\np_sum = p0 + p1\np0 /= p_sum\np1 /= p_sum\n\nprint \"probability of not playing: {}\".format(p0)\nprint \"probability of playing    : {}\".format(p1)", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "<a id = \"nbc2\"></a>\n### <span style=\"color:#0b486b\">5.2 NBC Exercise</span> \n\n\n\nSuppose we have documents below as our training set. \n\n    d1: Chinese Beijing Chinese , class = C\n    d2: Chinese Chinese Shanghai, class = C\n    d3: Chinese Macao           , class = C\n    d4: Tokyo Japan Chinese     , class = J\n\n\nTrain a NB classifier and predict if `d5` belongs to class C or J.\n\n    d5: Chinese Chinese Chinese Tokyo Japan, class = ?", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "# Your code here...", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "<div align=\"right\">\n<a href=\"#mat5\" class=\"btn btn-default\" data-toggle=\"collapse\">Click here for the solution</a>\n</div>\n<div id=\"mat5\" class=\"collapse\">\n```\nTBA\n```\n</div>", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "<a id = \"rf1\"></a>\n\n## 6. Random Forest\n\nRandom forests are an example of an *ensemble learner* built on decision trees.\nFor this reason we'll start by discussing decision trees themselves.\n\nDecision trees are extremely intuitive ways to classify or label objects: you simply ask a series of questions designed to zero-in on the classification.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "%matplotlib inline\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn; \nfrom sklearn.linear_model import LinearRegression\nfrom scipy import stats\nimport pylab as pl\n\nseaborn.set()", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "import fig_code\nfig_code.plot_example_decision_tree()", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "The binary splitting makes this extremely efficient.\nAs always, though, the trick is to *ask the right questions*.\nThis is where the algorithmic process comes in: in training a decision tree classifier, the algorithm looks at the features and decides which questions (or \"splits\") contain the most information.\n\n<a id = \"rf2\"></a>\n\n### 6.1 Creating a Decision Tree\n\nHere's an example of a decision tree classifier in scikit-learn. We'll start by defining some two-dimensional labeled data:", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "from sklearn.datasets import make_blobs\n\nX, y = make_blobs(n_samples=300, centers=4,\n                  random_state=0, cluster_std=1.0)\nplt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='rainbow');", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "# We have some convenience functions in the repository that help \nfrom fig_code import visualize_tree, plot_tree_interactive\n\n# Now using IPython's ``interact`` (available in IPython 2.0+, and requires a live kernel) we can view the decision tree splits:\nplot_tree_interactive(X, y);", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "Notice that at each increase in depth, every node is split in two **except** those nodes which contain only a single class.\nThe result is a very fast **non-parametric** classification, and can be extremely useful in practice.\n\n**Question: Do you see any problems with this?**\n\n<a id = \"rf3\"></a>\n\n### 6.2 Decision Trees and over-fitting\n\nOne issue with decision trees is that it is very easy to create trees which **over-fit** the data. That is, they are flexible enough that they can learn the structure of the noise in the data rather than the signal! For example, take a look at two trees built on two subsets of this dataset:", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "from sklearn.tree import DecisionTreeClassifier\nclf = DecisionTreeClassifier()\n\nplt.figure()\nvisualize_tree(clf, X[:200], y[:200], boundaries=False)\nplt.figure()\nvisualize_tree(clf, X[-200:], y[-200:], boundaries=False)", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "The details of the classifications are completely different! That is an indication of **over-fitting**: when you predict the value for a new point, the result is more reflective of the noise in the model rather than the signal.\n\n<a id = \"rf4\"></a>\n\n### 6.3 Ensembles of Estimators: Random Forests\n\nOne possible way to address over-fitting is to use an **Ensemble Method**: this is a meta-estimator which essentially averages the results of many individual estimators which over-fit the data. Somewhat surprisingly, the resulting estimates are much more robust and accurate than the individual estimates which make them up!\n\nOne of the most common ensemble methods is the **Random Forest**, in which the ensemble is made up of many decision trees which are in some way perturbed.\n\nThere are volumes of theory and precedent about how to randomize these trees, but as an example, let's imagine an ensemble of estimators fit on subsets of the data. We can get an idea of what these might look like as follows:", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "def fit_randomized_tree(random_state=0):\n    X, y = make_blobs(n_samples=300, centers=4,\n                      random_state=0, cluster_std=2.0)\n    clf = DecisionTreeClassifier(max_depth=15)\n    \n    rng = np.random.RandomState(random_state)\n    i = np.arange(len(y))\n    rng.shuffle(i)\n    visualize_tree(clf, X[i[:250]], y[i[:250]], boundaries=False,\n                   xlim=(X[:, 0].min(), X[:, 0].max()),\n                   ylim=(X[:, 1].min(), X[:, 1].max()))\n    \nfrom IPython.html.widgets import interact\ninteract(fit_randomized_tree, random_state=[0, 100]);", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "See how the details of the model change as a function of the sample, while the larger characteristics remain the same!\nThe random forest classifier will do something similar to this, but use a combined version of all these trees to arrive at a final answer:", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "from sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(n_estimators=100, random_state=0, n_jobs=-1)\nvisualize_tree(clf, X, y, boundaries=False);", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "By averaging over 100 randomly perturbed models, we end up with an overall model which is a much better fit to our data!\n\n*(Note: above we randomized the model through sub-sampling... Random Forests use more sophisticated means of randomization, which you can read about in, e.g. the [scikit-learn documentation](http://scikit-learn.org/stable/modules/ensemble.html#forest)*)\n\nNot good for random forest:\nlots of 0, few 1\nstructured data like images, neural network might be better\nsmall data, might overfit\nhigh dimensional data, linear model might work better\n\n<a id = \"rf5\"></a>\n\n### 6.4 Random Forest Regressor\n\nAbove we were considering random forests within the context of classification.\nRandom forests can also be made to work in the case of regression (that is, continuous rather than categorical variables). The estimator to use for this is ``sklearn.ensemble.RandomForestRegressor``.\n\nLet's quickly demonstrate how this can be used:", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "from sklearn.ensemble import RandomForestRegressor\n\nx = 10 * np.random.rand(100)\n\ndef model(x, sigma=0.3):\n    fast_oscillation = np.sin(5 * x)\n    slow_oscillation = np.sin(0.5 * x)\n    noise = sigma * np.random.randn(len(x))\n\n    return slow_oscillation + fast_oscillation + noise\n\ny = model(x)\nplt.errorbar(x, y, 0.3, fmt='o');", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "xfit = np.linspace(0, 10, 1000)\nyfit = RandomForestRegressor(100).fit(x[:, None], y).predict(xfit[:, None])\nytrue = model(xfit, 0)\n\nplt.errorbar(x, y, 0.3, fmt='o')\nplt.plot(xfit, yfit, '-r');\nplt.plot(xfit, ytrue, '-k', alpha=0.5);", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "As you can see, the non-parametric random forest model is flexible enough to fit the multi-period data, without us even specifying a multi-period model!\n\nTradeoff between simplicity and thinking about what your data is.\n\nFeature engineering is important, need to know your domain: Fourier transform frequency distribution.\n\n<a id = \"rf6\"></a>\n\n### 6.5 Random Forest Limitations\n\nThe following data scenarios are not well suited for random forests:\n* y: lots of 0, few 1\n* Structured data like images where a neural network might be better\n* Small data size which might lead to overfitting\n* High dimensional data where a linear model might work better", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "---\n## <span style=\"color:#0b486b\">7. Confidence Interval</span> ", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "<a id = \"popsample\"></a>\n\n### <span style=\"color:#0b486b\">7.1 Population vs Sample</span> \n\nThe main difference between population and sample comes down to how observations are assigned to dataset. A **population** includes all of the elements from the dataset. A **sample** consists of one or more observations from the population. In other words **population** is the entire collection of the desired measurable characteristic that we would have, if we could collect it. \n\nFor example if suppose we want to find the average height of 2nd grade students in Australia. The population would be all the students who are studying in 2nd grade in Australia. But most probably we can not measure the height of all Australian 2nd grade students. It is not feasible. So what do we do? **We sample!**. We collect the height of some of Australian 2nd graders and based on that, we **estimate** the average height of the population (all of Australian 2nd grade students). The sample could be 2nd grade students of one class in one school, or multiple classes in multiple schools in one state, or  multiple schools in multiple states, or etc.\n\nA measurable statistic of a population (such as mean) is called a **parameter**. But a measurable characteristic of a sample is called **statistic**.\n\n----\n<a id = \"ci\"></a> \n\n### <span style=\"color:#0b486b\">6.2 Confidence Interval</span> \n\nAs stated in previous section, population parameter is unknown. Confidence interval is a type of interval estimate of a population parameter calculated from sample statistics. It is an interval estimate combined with a probability.\n\nFor the aforementioned example, it means that without collecting the height of all Aussie 2nd graders, we can estimate the average height by collecting a sample and using the below formula:\n\n\n$$Confidence\\, Interval=\\bar{X}\\pm z\\frac{s}{\\sqrt{n}}$$\n\n$s$ is the sample standard deviation, $n$ is the sample size, and $z$ is often read from a table.\n\n    confidence level (%)     z\n         \n            70              1.04 \n            75              1.15\n            80              1.28 \n            85              1.44 \n            90              1.645\n            92              1.75\n            95              1.96\n            96              2.05\n            98              2.33\n            99              2.58", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "Now lets use an example to clarify this concept.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "import numpy as np\nimport matplotlib.pyplot as plt\nimport scipy.stats as ss\nimport seaborn as sns\n\n%matplotlib inline\nsns.set()", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "First we create a population. Please note that we do not use the population in our computations. We use samples from it.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "mu, sigma = 10, 2\nn_population = 10000\npopulation = np.random.normal(mu, sigma, n_population)", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "fig, ax = plt.subplots()\nax.hist(population, bins=25)\nax.set_title(r\"population histogram, $\\mu={}$, $\\sigma={}$ \".format(mu, sigma))", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "Now we sample multiple times from this population.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "n_trials = 1000\nsample_size = 500\nsamples = np.zeros([n_trials, sample_size])\nsample_std = np.zeros(n_trials)\nsample_mean = np.zeros(n_trials)\n\nfor i in xrange(n_trials):\n    samples[i] = np.random.choice(population, size=sample_size, replace=False)\n    sample_mean[i] = samples[i].mean()\n    sample_std[i] = samples[i].std()", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "i = 200\nse = sample_std[i] / np.sqrt(sample_size)\ndist = ss.distributions.norm(sample_mean[i], se)\nz = 1.96\n\nx = np.linspace(9.5, 10.5, 100)\ny = dist.pdf(x)\n\nfig, ax = plt.subplots()\n    \nax.hist(sample_mean, normed=True, bins=20, label='sample means')\nax.plot(x, y, label=\"$N(\\mu_{i},se)$\")\nax.vlines(sample_mean[i] + z*se, 0, 5, label='$\\mu_i \\pm z\\sigma_i$')\nax.vlines(sample_mean[i] - z*se, 0, 5)\n\nax.set_title(\"distribution of sample statistics\\ni={}\".format(i))\nax.legend()\n", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }
    ], 
    "metadata": {
        "kernelspec": {
            "display_name": "Python 2 with Spark 2.1", 
            "name": "python2-spark21", 
            "language": "python"
        }, 
        "language_info": {
            "mimetype": "text/x-python", 
            "nbconvert_exporter": "python", 
            "version": "2.7.11", 
            "name": "python", 
            "pygments_lexer": "ipython2", 
            "file_extension": ".py", 
            "codemirror_mode": {
                "version": 2, 
                "name": "ipython"
            }
        }, 
        "anaconda-cloud": {}
    }, 
    "nbformat": 4
}