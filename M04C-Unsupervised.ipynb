{
    "nbformat_minor": 1, 
    "cells": [
        {
            "source": "# Modern Data Science \n**(Module 04: Machine Learning)**\n\n---\n- Materials in this module include resources collected from various open-source online repositories.\n- You are free to use, change and distribute this package.\n\nPrepared by and for \n**Student Members** |\n2006-2018 [TULIP Lab](http://www.tulip.org.au), Australia\n\n---\n\n\n# Session C - Unsupervised Learning\n\n\nThe purpose of this session is to demonstrate unsupervised learning algortihms.\n\n\n## Content\n\n### Part 1 K-Means\n\n1.1 [Generating Random Data](#data)\n\n1.2 [Setting Up K-means](#kmeans)\n\n1.3 [Creating the Visual Plot](#plot)\n\n1.4 [Clustering Iris Data](#iris)\n\n\n### Part 2 Hierarchical Clustering\n\n2.1 [Generating Random Data](#data2)\n\n2.2 [Agglomerative Clustering](#agc)\n\n2.3 [Dendrogram](#den)\n \n### Part 3 Density-based Clustering\n\n\n### Part 4 Feature Selection\n\n4.1 [Generating Random Data](#data3)\n\n4.2 [Variance Feature Selection](#varfs)\n\n4.3 [UniVariance Feature Selection](#uvarfs)\n\n### Part 5 Dimensionality Reduction and PCA\n\n---", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "---\n## <span style=\"color:#0b486b\">1. K-Means Clustering</span>\n\n\n### Import the following libraries:\n<ul>\n    <li> <b>random</b> </li>\n    <li> <b>numpy as np</b> </li>\n    <li> <b>matplotlib.pyplot as plt</b> </li>\n    <li> <b>KMeans from sklearn.cluster</b> </li>\n    <li> <b>make_blobs from sklearn.datasets.samples_generator</b> </li>\n</ul>\n<br>\nAlso run <b> %matplotlib inline </b> since we will be plotting in this section.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "source": "import random \nimport numpy as np \nimport matplotlib.pyplot as plt \nfrom sklearn.cluster import KMeans \nfrom sklearn.datasets.samples_generator import make_blobs \n%matplotlib inline", 
            "outputs": [], 
            "metadata": {}
        }, 
        {
            "source": "<a id = \"data\"></a>\n\n### <span style=\"color:#0b486b\">1.1 Generating Random Data</span>\n\nSo we will be creating our own dataset!\n\nFirst we need to set up a random seed. Use <b>numpy's random.seed()</b> function, where the seed will be set to <b>0</b> <br>ex. random.seed(0)", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "source": "np.random.seed(0)", 
            "outputs": [], 
            "metadata": {}
        }, 
        {
            "source": "Next we will be making <i> random clusters </i> of points by using the <b> make_blobs </b> class. The <b> make_blobs </b> class can take in many inputs, but we will be using these specific ones. <br> <br>\n<b> <u> Input </u> </b>\n<ul>\n    <li> <b>n_samples</b>: The total number of points equally divided among clusters. </li>\n    <ul> <li> Value will be: 5000 </li> </ul>\n    <li> <b>centers</b>: The number of centers to generate, or the fixed center locations. </li>\n    <ul> <li> Value will be: [[4, 4], [-2, -1], [2, -3],[1,1]] </li> </ul>\n    <li> <b>cluster_std</b>: The standard deviation of the clusters. </li>\n    <ul> <li> Value will be: 0.9 </li> </ul>\n</ul>\n<br>\n<b> <u> Output </u> </b>\n<ul>\n    <li> <b>X</b>: Array of shape [n_samples, n_features]. (Feature Matrix)</li>\n    <ul> <li> The generated samples. </li> </ul> \n    <li> <b>y</b>: Array of shape [n_samples]. (Response Vector)</li>\n    <ul> <li> The integer labels for cluster membership of each sample. </li> </ul>\n</ul>\n\n", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "source": "X, y = make_blobs(n_samples=5000, centers=[[4,4], [-2, -1], [2, -3], [1, 1]], cluster_std=0.9)", 
            "outputs": [], 
            "metadata": {}
        }, 
        {
            "source": "Display the scatter plot of the randomly generated data.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "source": "plt.scatter(X[:, 0], X[:, 1], marker='.')", 
            "outputs": [], 
            "metadata": {}
        }, 
        {
            "source": "\n<a id = \"kmeans\"></a>\n\n\n### <span style=\"color:#0b486b\">1.2 Setting up K-means</span>\n\nNow that we have our random data, let's set up our K-Means Clustering.\n\nThe KMeans class has many parameters that can be used, but we will be using these three:\n<ul>\n    <li> <b>init</b>: Initialization method of the centroids. </li>\n    <ul>\n        <li> Value will be: \"k-means++\" </li>\n        <li> k-means++: Selects initial cluster centers for k-mean clustering in a smart way to speed up convergence.</li>\n    </ul>\n    <li> <b>n\\_clusters</b>: The number of clusters to form as well as the number of centroids to generate. </li>\n    <ul> <li> Value will be: 4 (since we have 4 centers)</li> </ul>\n    <li> <b>n\\_init</b>: Number of time the k-means algorithm will be run with different centroid seeds. The final results will be the best output of n\\_init consecutive runs in terms of inertia. </li>\n    <ul> <li> Value will be: 12 </li> </ul>\n</ul>\n\nInitialize KMeans with these parameters, where the output parameter is called <b>k_means</b>.\n", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "source": "k_means = KMeans(init = \"k-means++\", n_clusters = 4, n_init = 12)", 
            "outputs": [], 
            "metadata": {}
        }, 
        {
            "source": "Now let's fit the KMeans model with the feature matrix we created above, <b> X </b>", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "source": "k_means.fit(X)", 
            "outputs": [], 
            "metadata": {}
        }, 
        {
            "source": "Now let's grab the labels for each point in the model using KMeans' <b> .labels\\_ </b> attribute and save it as <b> k_means_labels </b> ", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "source": "k_means_labels = k_means.labels_", 
            "outputs": [], 
            "metadata": {}
        }, 
        {
            "source": "We will also get the coordinates of the cluster centers using KMeans' <b> .cluster&#95;centers&#95; </b> and save it as <b> k_means_cluster_centers </b>", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "source": "k_means_cluster_centers = k_means.cluster_centers_", 
            "outputs": [], 
            "metadata": {}
        }, 
        {
            "source": "\n<a id = \"plot\"></a>\n\n\n### <span style=\"color:#0b486b\">1.3 Creating the Visual Plot</span>\n\nSo now that we have the random data generated and the KMeans model initialized, let's plot them and see what it looks like!\n\nPlease read through the code and comments to understand how to plot the model.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "source": "# Initialize the plot with the specified dimensions.\nfig = plt.figure(figsize=(6, 4))\n\n# Colors uses a color map, which will produce an array of colors based on\n# the number of labels there are. We use set(k_means_labels) to get the\n# unique labels.\ncolors = plt.cm.Spectral(np.linspace(0, 1, len(set(k_means_labels))))\n\n# Create a plot with a black background (background is black because we can see the points\n# connection to the centroid.\nax = fig.add_subplot(1, 1, 1, axisbg = 'black')\n\n# For loop that plots the data points and centroids.\n# k will range from 0-3, which will match the possible clusters that each\n# data point is in.\nfor k, col in zip(range(len([[2, 2], [-2, -1], [4, -3], [1, 1]])), colors):\n\n    # Create a list of all data points, where the data poitns that are \n    # in the cluster (ex. cluster 0) are labeled as true, else they are\n    # labeled as false.\n    my_members = (k_means_labels == k)\n    \n    # Define the centroid, or cluster center.\n    cluster_center = k_means_cluster_centers[k]\n    \n    # Plots the datapoints with color col.\n    ax.plot(X[my_members, 0], X[my_members, 1], 'w',\n            markerfacecolor=col, marker='.')\n    \n    # Plots the centroids with specified color, but with a darker outline\n    ax.plot(cluster_center[0], cluster_center[1], 'o', markerfacecolor=col,\n            markeredgecolor='k', markersize=6)\n\n# Title of the plot\nax.set_title('KMeans')\n\n# Remove x-axis ticks\nax.set_xticks(())\n\n# Remove y-axis ticks\nax.set_yticks(())\n\n# Show the plot\nplt.show()\n\n# Display the scatter plot from above for comparison.\nplt.scatter(X[:, 0], X[:, 1], marker='.')", 
            "outputs": [], 
            "metadata": {}
        }, 
        {
            "source": "\n<a id = \"iris\"></a>\n\n\n### <span style=\"color:#0b486b\">1.4 Clustering Iris Data</span>\n\nImport the following libraries: \n<ol>- Axes3D from mpl_toolkits.mplot3d</ol>\n<ol>- KMeans from sklearn.cluster</ol>\n<ol>- load_iris from sklearn.datasets</ol>\n\n<i>Note: It is presumed that numpy and matplotlib.pyplot are both imported as np and plt respectively from previous imports. If that is not the case, please import them!</i>", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "source": "from mpl_toolkits.mplot3d import Axes3D \nfrom sklearn.cluster import KMeans \nfrom sklearn.datasets import load_iris", 
            "outputs": [], 
            "metadata": {}
        }, 
        {
            "source": "Then we will set the <b>random seed</b> and the <b>centers</b> for <b>K-means</b>.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "source": "np.random.seed(5)\n\ncenters = [[1, 1], [-1, -1], [1, -1]]", 
            "outputs": [], 
            "metadata": {}
        }, 
        {
            "source": "Using the <b> load_iris() </b> function, declare the iris datset as the variable <b>iris</b>", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "source": "iris = load_iris()", 
            "outputs": [], 
            "metadata": {}
        }, 
        {
            "source": "Also declare <b>X</b> as the <b>iris' data component</b>, and y as <b>iris' target component</b>", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "source": "X = iris.data \ny = iris.target", 
            "outputs": [], 
            "metadata": {}
        }, 
        {
            "source": "Now let's run the rest of the code and see what <b>K-Means produces!</b>", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "source": "estimators = {'k_means_iris_3': KMeans(n_clusters=3),\n              'k_means_iris_8': KMeans(n_clusters=8),\n              'k_means_iris_bad_init': KMeans(n_clusters=3, n_init=1,\n                                              init='random')}\n\nfignum = 1\nfor name, est in estimators.items():\n    fig = plt.figure(fignum, figsize=(4, 3))\n    plt.clf()\n    ax = Axes3D(fig, rect=[0, 0, .95, 1], elev=48, azim=134)\n\n    plt.cla()\n    est.fit(X)\n    labels = est.labels_\n\n    ax.scatter(X[:, 3], X[:, 0], X[:, 2], c=labels.astype(np.float))\n\n    ax.w_xaxis.set_ticklabels([])\n    ax.w_yaxis.set_ticklabels([])\n    ax.w_zaxis.set_ticklabels([])\n    ax.set_xlabel('Petal width')\n    ax.set_ylabel('Sepal length')\n    ax.set_zlabel('Petal length')\n    fignum = fignum + 1\n\n# Plot the ground truth\nfig = plt.figure(fignum, figsize=(4, 3))\nplt.clf()\nax = Axes3D(fig, rect=[0, 0, .95, 1], elev=48, azim=134)\n\nplt.cla()\n\nfor name, label in [('Setosa', 0),\n                    ('Versicolour', 1),\n                    ('Virginica', 2)]:\n    ax.text3D(X[y == label, 3].mean(),\n              X[y == label, 0].mean() + 1.5,\n              X[y == label, 2].mean(), name,\n              horizontalalignment='center',\n              bbox=dict(alpha=.5, edgecolor='w', facecolor='w'))\n# Reorder the labels to have colors matching the cluster results\ny = np.choose(y, [1, 2, 0]).astype(np.float)\nax.scatter(X[:, 3], X[:, 0], X[:, 2], c=y)\n\nax.w_xaxis.set_ticklabels([])\nax.w_yaxis.set_ticklabels([])\nax.w_zaxis.set_ticklabels([])\nax.set_xlabel('Petal width')\nax.set_ylabel('Sepal length')\nax.set_zlabel('Petal length')\nplt.show()", 
            "outputs": [], 
            "metadata": {}
        }, 
        {
            "source": "The following <b>plots</b> (1-3) show the different <b>end results</b> you obtain by using different <b>initalization processes</b>. <b>Plot 4</b> holds what the answer should be, however it is clear that <b>K-means</b> is <b>heavily reliant</b> on the <b>initalization</b> of the <b>centroid</b>.\n\n", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "---\n## <span style=\"color:#0b486b\">2. Hierarchical Clustering</span>\n\n\nWe will be looking at the next clustering technique, which is <b>Agglomerative Hierarchical Clustering</b>. Remember that agglomerative is the bottom up approach. <br> <br>\nIn this lab, we will be looking at Agglomerative clustering, which is more popular than Divisive clustering. <br> <br>\nWe will also be using Complete Linkage as the Linkage Criteria. <br>\n<b> <i> NOTE: You can also try using Average Linkage wherever Complete Linkage would be used to see the difference! </i> </b>\n\n---\nImport Libraries:\n<ul>\n    <li> <b>numpy as np</b> </li>\n    <li> <b>ndimage</b> from <b>scipy</b> </li>\n    <li> <b>hierarchy</b> from <b>scipy.cluster</b> </li>\n    <li> <b>pyplot as plt</b> from <b>matplotlib</b> </li>\n    <li> <b>manifold</b> from <b>sklearn</b> </li>\n    <li> <b>datasets</b> from <b>sklearn</b> </li>\n    <li> <b>AgglomerativeClustering</b> from <b>sklearn</b> </li>\n    <li> <b>make_blobs</b> from <b>sklearn.datasets.samples_generator</b> </li>\n</ul> <br>\nAlso run <b>%matplotlib inline</b> that that wasn't run already.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "source": "import numpy as np \nfrom scipy import ndimage \nfrom scipy.cluster import hierarchy \nfrom scipy.spatial import distance_matrix \nfrom matplotlib import pyplot as plt \nfrom sklearn import manifold, datasets \nfrom sklearn.cluster import AgglomerativeClustering \nfrom sklearn.datasets.samples_generator import make_blobs \n%matplotlib inline", 
            "outputs": [], 
            "metadata": {}
        }, 
        {
            "source": "<a id = \"data2\"></a>\n\n### <span style=\"color:#0b486b\">2.1 Generating Random Data</span>\n\nWe will be generating another set of data using the <b>make_blobs</b> class once again. This time you will input your own values! <br> <br>\nInput these parameters into make_blobs:\n<ul>\n    <li> <b>n_samples</b>: The total number of points equally divided among clusters. </li>\n    <ul> <li> Choose a number from 10-1500 </li> </ul>\n    <li> <b>centers</b>: The number of centers to generate, or the fixed center locations. </li>\n    <ul> <li> Choose arrays of x,y coordinates for generating the centers. Have 1-10 centers (ex. centers=[[1,1], [2,5]]) </li> </ul>\n    <li> <b>cluster_std</b>: The standard deviation of the clusters. The larger the number, the further apart the clusters</li>\n    <ul> <li> Choose a number between 0.5-1.5 </li> </ul>\n</ul> <br>\nSave the result to <b>X2</b> and <b>y2</b>.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "source": "X2, y2 = make_blobs(n_samples=50, centers=[[4,4], [-2, -1], [1, 1], [10,4]], cluster_std=0.9)", 
            "outputs": [], 
            "metadata": {}
        }, 
        {
            "source": "Plot the scatter plot of the randomly generated data", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "source": "plt.scatter(X2[:, 0], X2[:, 1], marker='.') ", 
            "outputs": [], 
            "metadata": {}
        }, 
        {
            "source": "---\n<a id = \"agc\"></a>\n\n### <span style=\"color:#0b486b\">2.2 Agglomerative Clustering</span>\n\nWe will start by clustering the random data points we just created.\n\nThe <b> AgglomerativeClustering </b> class will require two inputs:\n<ul>\n    <li> <b>n_clusters</b>: The number of clusters to form as well as the number of centroids to generate. </li>\n    <ul> <li> Value will be: 4 </li> </ul>\n    <li> <b>linkage</b>: Which linkage criterion to use. The linkage criterion determines which distance to use between sets of observation. The algorithm will merge the pairs of cluster that minimize this criterion. </li>\n    <ul> \n        <li> Value will be: 'complete' </li> \n        <li> <b>Note</b>: It is recommended you try everything with 'average' as well </li>\n    </ul>\n</ul> <br>\nSave the result to a variable called <b> agglom </b>", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "source": "agglom = AgglomerativeClustering(n_clusters = 4, linkage = 'average')", 
            "outputs": [], 
            "metadata": {}
        }, 
        {
            "source": "Fit the model with <b> X2 </b> and <b> y2 </b> from the generated data above.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "source": "agglom.fit(X2,y2)", 
            "outputs": [], 
            "metadata": {}
        }, 
        {
            "source": "Run the following code to show the clustering! <br>\nRemember to read the code and comments to gain more understanding on how the plotting works.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "source": "# Create a figure of size 6 inches by 4 inches.\nplt.figure(figsize=(6,4))\n\n# These two lines of code are used to scale the data points down,\n# Or else the data points will be scattered very far apart.\n\n# Create a minimum and maximum range of X2.\nx_min, x_max = np.min(X2, axis=0), np.max(X2, axis=0)\n\n# Get the average distance for X2.\nX2 = (X2 - x_min) / (x_max - x_min)\n\n# This loop displays all of the datapoints.\nfor i in range(X2.shape[0]):\n    # Replace the data points with their respective cluster value \n    # (ex. 0) and is color coded with a colormap (plt.cm.spectral)\n    plt.text(X2[i, 0], X2[i, 1], str(y2[i]),\n             color=plt.cm.spectral(agglom.labels_[i] / 10.),\n             fontdict={'weight': 'bold', 'size': 9})\n    \n# Remove the x ticks, y ticks, x and y axis\nplt.xticks([])\nplt.yticks([])\nplt.axis('off')\n\n# Display the plot\nplt.show()\n\n# Display the plot of the original data before clustering\nplt.scatter(X2[:, 0], X2[:, 1], marker='.')", 
            "outputs": [], 
            "metadata": {}
        }, 
        {
            "source": "---\n<a id = \"den\"></a>\n\n### <span style=\"color:#0b486b\">2.3 Dendrogram</span>\n\n\nRemember that a <b>distance matrix</b> contains the <b> distance from each point to every other point of a dataset </b>. <br>\nUse the function <b> distance_matrix, </b> which requires <b>two inputs</b>. Use the Feature Matrix, <b> X2 </b> as both inputs and save the distance matrix to a variable called <b> dist_matrix </b> <br> <br>\nRemember that the distance values are symmetric, with a diagonal of 0's. This is one way of making sure your matrix is correct. <br> (print out dist_matrix to make sure it's correct)", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "source": "dist_matrix = distance_matrix(X2,X2) \nprint(dist_matrix)", 
            "outputs": [], 
            "metadata": {}
        }, 
        {
            "source": "Using the <b> linkage </b> class from hierarchy, pass in the parameters:\n<ul>\n    <li> The distance matrix </li>\n    <li> 'complete' for complete linkage </li>\n</ul> <br>\nSave the result to a variable called <b> Z </b>", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "source": "Z = hierarchy.linkage(dist_matrix, 'complete')", 
            "outputs": [], 
            "metadata": {}
        }, 
        {
            "source": "Next, we will save the dendrogram to a variable called <b>dendro</b>. In doing this, the dendrogram will also be displayed.\nUsing the <b> dendrogram </b> class from hierarchy, pass in the parameter:\n<ul> <li> Z </li> </ul>", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "source": "dendro = hierarchy.dendrogram(Z)", 
            "outputs": [], 
            "metadata": {}
        }, 
        {
            "source": "---\n## <span style=\"color:#0b486b\">3. Density-based Clustering</span>\n\n\nWe will be looking at the next clustering technique, which is <b>Agglomerative Hierarchical Clustering</b>. Remember that agglomerative is the bottom up approach. <br> <br>\nIn this lab, we will be looking at Agglomerative clustering, which is more popular than Divisive clustering. <br> <br>\nWe will also be using Complete Linkage as the Linkage Criteria. <br>\n<b> <i> NOTE: You can also try using Average Linkage wherever Complete Linkage would be used to see the difference! </i> </b>\n\n---\nImport Libraries:\n<ul>\n    <li> <b>numpy as np</b> </li>\n    <li> <b>ndimage</b> from <b>scipy</b> </li>\n    <li> <b>hierarchy</b> from <b>scipy.cluster</b> </li>\n    <li> <b>pyplot as plt</b> from <b>matplotlib</b> </li>\n    <li> <b>manifold</b> from <b>sklearn</b> </li>\n    <li> <b>datasets</b> from <b>sklearn</b> </li>\n    <li> <b>AgglomerativeClustering</b> from <b>sklearn</b> </li>\n    <li> <b>make_blobs</b> from <b>sklearn.datasets.samples_generator</b> </li>\n</ul> <br>\nAlso run <b>%matplotlib inline</b> that that wasn't run already.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "source": "import numpy as np \nfrom sklearn.cluster import DBSCAN \nfrom sklearn.datasets.samples_generator import make_blobs \nfrom sklearn.preprocessing import StandardScaler \nimport matplotlib.pyplot as plt \n%matplotlib inline", 
            "outputs": [], 
            "metadata": {}
        }, 
        {
            "source": "The function below will generate the data points and requires these inputs:\n<ul>\n    <li> <b>centroidLocation</b>: Coordinates of the centroids that will generate the random data. </li>\n    <ul> <li> Example: input: [[4,3], [2,-1], [-1,4]] </li> </ul>\n    <li> <b>numSamples</b>: The number of data points we want generated, split over the number of centroids (# of centroids defined in centroidLocation) </li>\n    <ul> <li> Example: 1500 </li> </ul>\n    <li> <b>clusterDeviation</b>: The standard deviation between the clusters. The larger the number, the further the spacing. </li>\n    <ul> <li> Example: 0.5 </li> </ul>\n</ul>", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "source": "def createDataPoints(centroidLocation, numSamples, clusterDeviation):\n    # Create random data and store in feature matrix X and response vector y.\n    X, y = make_blobs(n_samples=numSamples, centers=centroidLocation, \n                                cluster_std=clusterDeviation)\n    \n    # Standardize features by removing the mean and scaling to unit variance\n    X = StandardScaler().fit_transform(X)\n    return X, y", 
            "outputs": [], 
            "metadata": {}
        }, 
        {
            "source": "The function below will generate the DBSCAN using the input data:\n<ul>\n    <li> <b>epsilon</b>: A float that describes the maximum distance between two samples for them to be considered as in the same neighborhood. </li>\n    <ul> <li> Example: 0.3 </li> </ul>\n    <li> <b>minimumSamples</b>: The number of samples (or total weight) in a neighborhood for a point to be considered as a core point. This includes the point itself. </li>\n    <ul> <li> Examples: 7 </li> </ul>\n</ul>", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "source": "def displayDBSCAN(epsilon, minimumSamples):\n    \n    # Initialize DBSCAN with specified epsilon and min. smaples. Fit the model with feature\n    # matrix X\n    db = DBSCAN(eps=epsilon, min_samples=minimumSamples).fit(X)\n    \n    # Create an array of booleans using the labels from db.\n    core_samples_mask = np.zeros_like(db.labels_, dtype=bool)\n    \n    # Replace all elements with 'True' in core_samples_mask that are\n    # in the cluster, 'False' if the points are outliers.\n    core_samples_mask[db.core_sample_indices_] = True\n    labels = db.labels_\n\n    # Number of clusters in labels, ignoring noise if present.\n    n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)\n\n\n    # Black color is removed and used for noise instead.\n    \n    # Remove repetition in labels by turning it into a set.\n    unique_labels = set(labels)\n    \n    # Create colors for the clusters.\n    colors = plt.cm.Spectral(np.linspace(0, 1, len(unique_labels)))\n    \n    # Plot the points with colors\n    for k, col in zip(unique_labels, colors):\n        if k == -1:\n            # Black used for noise.\n            col = 'k'\n\n        class_member_mask = (labels == k)\n        \n        # Plot the datapoints that are clustered\n        xy = X[class_member_mask & core_samples_mask]\n        plt.plot(xy[:, 0], xy[:, 1], 'o', markerfacecolor=col,\n                 markeredgecolor='k', markersize=14)\n\n        # Plot the outliers\n        xy = X[class_member_mask & ~core_samples_mask]\n        plt.plot(xy[:, 0], xy[:, 1], 'o', markerfacecolor=col,\n                 markeredgecolor='k', markersize=6)\n\n    plt.title('Estimated number of clusters: %d' % n_clusters_)\n    plt.show()", 
            "outputs": [], 
            "metadata": {}
        }, 
        {
            "source": "Use <b>createDataPoints</b> with the <b>3 inputs</b> and store the output into variables <b>X</b> and <b>y</b>.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "source": "createDataPoints([[4,3], [2,-1], [-1,4]] , 1500, 0.5)", 
            "outputs": [], 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "source": "displayDBSCAN(0.3, 7)", 
            "outputs": [], 
            "metadata": {}
        }, 
        {
            "source": "---\n## <span style=\"color:#0b486b\">4. Feature Selection</span>\n\n\nIn this lab exercise, you will learn how to use <b>Dimensionality Reduction</b> in the form of <b>Feature Selection</b> and <b>Feature Extraction</b>.\n\nWe will first be looking at Feature Selection with <b>VarianceThreshold</b>. VarianceThreshold is a useful tool to removing features with a threshold variance. It is a simple and basic Feature Selection.\n\n---\n<a id = \"data3\"></a>\n\n### <span style=\"color:#0b486b\">4.1 Data Set</span>\n\nNow we will be working with the <b>skulls dataset</b> once again. Using the <b>my_data</b> variable and <b>removeColumns</b> function, create a variable called <b>X</b> which has the <b>row column dropped</b>.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "source": "import wget\n\nlink_to_data = 'https://github.com/tuliplab/mds/raw/master/Jupyter/data/skulls.csv'\nDataSet = wget.download(link_to_data)", 
            "outputs": [], 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "source": "import pandas\n\nmy_data = pandas.read_csv(\"skulls.csv\", delimiter=\",\")", 
            "outputs": [], 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "source": "# Remove the column containing the target name since it doesn't contain numeric values.\n# Also remove the column that contains the row number\n# axis=1 means we are removing columns instead of rows.\n# Function takes in a pandas array and column numbers and returns a numpy array without\n# the stated columns\ndef removeColumns(pandasArray, *column):\n    return pandasArray.drop(pandasArray.columns[[column]], axis=1).values", 
            "outputs": [], 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "source": "X = removeColumns(my_data, 0, 1)", 
            "outputs": [], 
            "metadata": {}
        }, 
        {
            "source": "Now use the <b>target function</b> to obtain the <b>Response Vector</b> of <b>my_data</b> and store it as <b>y</b>", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "source": "def target(numpyArray, targetColumnIndex):\n    target_dict = dict()\n    target = list()\n    count = -1\n    for i in range(len(my_data.values)):\n        if my_data.values[i][targetColumnIndex] not in target_dict:\n            count += 1\n            target_dict[my_data.values[i][targetColumnIndex]] = count\n        target.append(target_dict[my_data.values[i][targetColumnIndex]])\n    return np.asarray(target)", 
            "outputs": [], 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "source": "y = target(my_data, 1)", 
            "outputs": [], 
            "metadata": {}
        }, 
        {
            "source": "---\n<a id = \"varfs\"></a>\n\n### <span style=\"color:#0b486b\">4.2 Variance Feature Selection</span>\n\nFirst import <b>VarianceThreshold</b> from sklearn.feature_selection", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "source": "from sklearn.feature_selection import VarianceThreshold", 
            "outputs": [], 
            "metadata": {}
        }, 
        {
            "source": "Now let's instantiate <b>VarianceThreshold</b> as a variable called <b>sel</b>", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "source": "sel = VarianceThreshold()", 
            "outputs": [], 
            "metadata": {}
        }, 
        {
            "source": "Now <b>VarianceThreshold</b> removes all <b>zero-variance features</b> by default. These features are any <b>constant value</b> features. Given the dataset below, let's try to run <b>fit_transform</b> function from <b>sel</b> on it.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "source": "sel.fit_transform(X)", 
            "outputs": [], 
            "metadata": {}
        }, 
        {
            "source": "Now you should have only <b>two features</b> left. The first and second features were removed since they had a <b>variance</b> of 0. You probably won't encounter constant value features very often, therefore you will want to keep a certain <b>threshold</b>. \n\n\nWe can change the threshold by adding <b>threshold='threshold value'</b> inside the brackets during the instantiation of <b>VarianceThreshold</b>. Where <b>'threshold value'</b> is equal to <br>\n\n$$ Var(X) = p(1-p) $$\n\nWhere <b>'p'</b> is your threshold % in <b>decimal format</b>.\n\nSo, for example if I wanted a threshold of <b>60%</b>, I would equate <b>threshold=0.6 * (1 - 0.6)</b>\n\nNow let's instantiate another <b>VarianceThreshold</b> but with a threshold of <b>90%</b>. We'll called it <b>sel90</b>.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "source": "sel90 = VarianceThreshold(threshold=(0.9 * (1 - 0.9)))", 
            "outputs": [], 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "source": "sel90.fit_transform(X)", 
            "outputs": [], 
            "metadata": {}
        }, 
        {
            "source": "You should only have <b>one feature</b> left. The last column feature was the only feature to have a variance of <b>at least 90%</b>.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "---\n<a id = \"uvarfs\"></a>\n\n### <span style=\"color:#0b486b\">4.3 Univariance Feature Selection</span>\n\nNow let's look at <b>Univariance Feature Selection</b>.\n\nWe will need to import <b>SelectKBest</b> from <b>sklearn.feature_selection</b>, <b>chi2</b> from <b>sklearn.feature_selection</b>, <b>numpy</b> as <b>np</b>, and <b>pandas</b>.\n\nHow <b>Univariance</b> works is that it selects <b>features</b> based on <b>univariance statistical tests</b>. <b>chi2</b> is used as a <b>univariance scoring function</b> which returns <b>p</b> values. We specified <b>k=3</b> for the <b>3 best features</b> to be chosen. Now we will move onto <b>Feature Extraction!</b>", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "source": "from sklearn.feature_selection import SelectKBest \nfrom sklearn.feature_selection import chi2 \nimport numpy as np \nimport pandas", 
            "outputs": [], 
            "metadata": {}
        }, 
        {
            "source": "Now take a look at <b>X's shape</b> before the feature selection", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "source": "X.shape", 
            "outputs": [], 
            "metadata": {}
        }, 
        {
            "source": "Now we will use the <b>fit_transform</b> function with parameters <b>X</b>, <b>y</b> of <b>SelectKBest</b> with parameters <b>chi2</b>, <b>k=3</b>. This will be stored as <b>X_new</b>.\n<br><br>\n<b>Note</b>: There is a VisibleDeprecationWarning, you can ignore it.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "source": "X_new = SelectKBest(chi2, k=3).fit_transform(X, y)", 
            "outputs": [], 
            "metadata": {}
        }, 
        {
            "source": "Now let's check out the shape of <b>X_new</b>, it should have <b>one less</b> feature than before!", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "source": "X_new.shape", 
            "outputs": [], 
            "metadata": {}
        }, 
        {
            "source": "---\n## <span style=\"color:#0b486b\">5. Dimensionality Reduction and PCA</span>\n\n<b>DictVectorizer</b> is a very simple <b>Feature Extraction</b> class as it can be used to <b>convert feature arrays</b> in a <b>dict</b> to <b>NumPy/SciPy</b> representations.\n\nFirst we will need to import <b>DictVectorizer</b> from <b>sklearn.feature_extraction</b>\n\n", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "source": "from sklearn.feature_extraction import DictVectorizer", 
            "outputs": [], 
            "metadata": {}
        }, 
        {
            "source": "We will use the following <b>dictionary</b> to be <b>converted</b>.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "source": "dataset = [\n...     {'Day': 'Monday', 'Temperature': 18},\n...     {'Day': 'Tuesday', 'Temperature': 13},\n...     {'Day': 'Wednesday', 'Temperature': 7},\n... ]", 
            "outputs": [], 
            "metadata": {}
        }, 
        {
            "source": "Now create an <b>instance</b> of <b>DictVectorizer</b> called <b>vec</b>", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "source": "vec = DictVectorizer()", 
            "outputs": [], 
            "metadata": {}
        }, 
        {
            "source": "Now we will use the <b>fit_transform</b> function of <b>vec</b> with the parameter <b>dataset</b> and use the <b>.toarray()</b> on the final product", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "source": "vec.fit_transform(dataset).toarray()", 
            "outputs": [], 
            "metadata": {}
        }, 
        {
            "source": "Now we can see that our <b>dataset</b> has been <b>converted</b> into an <b>array format</b> but pertaining its <b>data</b>. We can further review the <b>data</b> with the <b>get_feature_names</b> function of <b>vec</b>.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "source": "vec.get_feature_names()", 
            "outputs": [], 
            "metadata": {}
        }, 
        {
            "source": "Now we will use <b>PCA</b> to represent the <b>data</b> we used in feature selection(<b>X_new</b>) and project it's dimensions so make sure you have completed that portion! But first we must import <b>matplotlib.pyplot</b> as <b>plt</b>, <b>Axes3D</b> from <b>mpl_toolkits.mplot3d</b>, and <b>decomposition</b> from <b>sklearn</b>. <i>(And numpy as np if you haven't already!)</i> Make sure you include the <b>%matplotlib inline</b> to have the plot show up in your notebook!", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "source": "import matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\nfrom sklearn import decomposition\n%matplotlib inline", 
            "outputs": [], 
            "metadata": {}
        }, 
        {
            "source": "Now we will just create instances of <b>plt.figure</b> as <b>fig</b> and <b>Axes3D</b> as <b>ax</b> with the following dimensions. In this case we specificed dimensions where <b>elev=0</b> and <b>azim=0</b> to see the graph from where the <b>z plane = 0</b>. This will be useful to <b>visualize</b> the <b>difference</b> later on.\n\n<i> Note: You can change the elev and azim later on to see the graph in different ways. But it is suggested to keep it as is for now.</i>\n\nWe will plot <b>X_new</b> against <b>y</b> with the <b>scatter</b> function of <b>ax</b>. The <b>scatter</b> function will include all of <b>X_new's column data</b> (each as a seperate parameter), <b>c=y</b>, and <b>cmap=plt.cm.spectral</b> as below.\n\n<b>Note</b>: You can ignore the FutureWarning.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "source": "fig = plt.figure(1, figsize=(10, 8))\nax = Axes3D(fig, rect=[0, 0, .95, 1], elev=0, azim=0)\nax.scatter(X_new[:, 0], X_new[:, 1], X_new[:, 2], c=y, cmap=plt.cm.spectral)", 
            "outputs": [], 
            "metadata": {}
        }, 
        {
            "source": "Now you know what a <b>3D representation</b> of that data looks like from a <b>z = 0 plane perspective</b>, we will create an instance of <b>decomposition.PCA</b> called <b>pca</b> with parameters of <b>n_components=2</b>.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "source": "pca = decomposition.PCA(n_components=2)", 
            "outputs": [], 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "source": "pca.fit(X_new)", 
            "outputs": [], 
            "metadata": {}
        }, 
        {
            "source": "Use the <b>transform</b> function of <b>pca</b> with parameter <b>X_new</b> and equate it to a new variable called <b>PCA_X</b>. This will be the <b>projection</b> resulting in the change of <b>3 features</b> to <b>2</b>.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "source": "PCA_X = pca.transform(X_new)", 
            "outputs": [], 
            "metadata": {}
        }, 
        {
            "source": "Now plot <b>PCA_X</b> with <b>y</b> using the <b>scatter</b> function of <b>ax</b> as we did before. All the <b>parameters</b> will be the <b>same</b> except you are using <b>PCA_X</b> and the <b>highest</b> index column you are going to is <b>PCA_X[:, 1]</b>. Make sure you include the <b>fig</b> and <b>ax</b> declaration to make the graph show up!", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "source": "fig = plt.figure(1, figsize=(10, 8))\nax = Axes3D(fig, rect=[0, 0, .95, 1], elev=0, azim=0)\nax.scatter(PCA_X[:, 0], PCA_X[:, 1], c=y, cmap=plt.cm.spectral)", 
            "outputs": [], 
            "metadata": {}
        }, 
        {
            "source": "Now as you can see, given the <b>same view</b>, there are <b>no datapoints</b> except on the <b>z = 0 plane</b>. Rather the <b>datapoints</b> are on the <b>other axes</b> confirming a <b>projection</b> from <b>3 features</b> to <b>2</b>. You can even look at the <b>shape</b> of <b>PCA_X</b> to show <b>2 features</b>.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "source": "PCA_X.shape", 
            "outputs": [], 
            "metadata": {}
        }, 
        {
            "source": "You may wish to <b>change</b> the values of <b>elev</b> and <b>azim</b> if you would like to view the graph in <b>different perspectives</b>. <b>Elev</b> controls the <b>elevation</b> of the <b>z plane</b> and <b>azim</b> controls the <b>azimuth angle</b> in the <b>x,y plane</b>.", 
            "cell_type": "markdown", 
            "metadata": {}
        }
    ], 
    "nbformat": 4, 
    "metadata": {
        "kernelspec": {
            "display_name": "Python 2 with Spark 2.1", 
            "name": "python2-spark21", 
            "language": "python"
        }, 
        "language_info": {
            "mimetype": "text/x-python", 
            "nbconvert_exporter": "python", 
            "version": "2.7.11", 
            "name": "python", 
            "file_extension": ".py", 
            "pygments_lexer": "ipython2", 
            "codemirror_mode": {
                "version": 2, 
                "name": "ipython"
            }
        }, 
        "anaconda-cloud": {}
    }
}